// This file is auto-generated. Do not edit manually.
import { BlogPost, Category } from './types';
import { authors } from './authors';

// Helper function to handle image paths based on environment
const getImagePath = (path: string) => {
  if (import.meta.env.PROD) {
    return path.replace(/^\/\/src/, '');
  }
  return path;
};

export const categories: Category[] = [
  {
    "value": "tutorials",
    "label": "Tutorials",
    "description": "Step-by-step guides and technical walkthroughs"
  },
  {
    "value": "news",
    "label": "News",
    "description": "Latest updates and announcements from Valkey"
  },
  {
    "value": "case-studies",
    "label": "Case Studies",
    "description": "Real-world examples and implementation stories"
  }
];

export const blogPostsRaw = [
  {
    "title": "A new hash table",
    "date": "2025-03-28T06:00:00.000Z",
    "excerpt": "Designing a state-of-the art hash table",
    "content": "<p>Many workloads are bound on storing data. Being able to store more data using\nless memory allows you to reduce the size of your clusters.</p>\n<p>In Valkey, keys and values are stored in what&#39;s called a hash table. A hash\ntable works by chopping a key into a number of seemingly random bits. These bits\nare shaped into a memory address, pointing to where the value is supposed to be\nstored. It&#39;s a very fast way of jumping directly to the right place in memory\nwithout scanning through all the keys.</p>\n<p>For the 8.1 release, we looked into improving the performance and memory usage,\nso that users can store more data using less memory. This work led us to the\ndesign of a new hash table, but first, let&#39;s take a look at the hash table that\nwas used in Valkey until now.</p>\n<h2>The dict</h2>\n<p>The hash table used Valkey until now, called &quot;dict&quot;, has the following memory\nlayout:</p>\n<p><img src=\"/content/blog/2025-03-28-new-hash-table/dict-structure.png\" alt=\"dict structure\" /></p>\n<p>The dict has two tables, called &quot;table 0&quot; and &quot;table 1&quot;. Usually only one\nexists, but both are used when incremental rehashing is in progress.</p>\n<p>It&#39;s a chained hash table, so if multiple keys are hashed to the same slot in\nthe table, their key-value entries form a linked list. That&#39;s what the &quot;next&quot;\npointer in the <code>dictEntry</code> is for.</p>\n<p>To lookup a key &quot;FOO&quot; and access the value &quot;BAR&quot;, Valkey still has to read from\nmemory four times. If there is a hash collision, it has to follow two more\npointers for each hash collision and thus read twice more from memory (the key\nand the next pointer).</p>\n<h2>Minimize memory accesses</h2>\n<p>One of the slower operations when looking up a key-value pair is reading from\nthe main RAM memory. A key point is therefore to make sure we have as few memory\naccesses as possible. Ideally, the memory we want to access should already be\nloaded in the CPU cache, which is a smaller but much faster memory belonging to\nthe CPU.</p>\n<p>Optimizing for memory usage, we also want to minimize the number of distinct\nmemory allocations and the number of pointers between them, because storing a\npointer needs 8 bytes in a 64-bit system. If we can save one pointer per\nkey-value pair, for 100 million keys that&#39;s almost a gigabyte.</p>\n<p>When the CPU loads some data from the main memory into the CPU cache, it does so\nin fixed size blocks called cache lines. The cache-line size is 64 bytes on\nalmost all modern hardware. Recent work on hash tables, such as <a href=\"https://abseil.io/about/design/swisstables\">Swiss\ntables</a>, are highly optimized to\nstore and access data within a single cache line. If the key you&#39;re not looking\nfor isn&#39;t found where you first look for it (due to a hash collision), then it\nshould ideally be found within the same cache line. If it is, then it&#39;s found\nvery fast once this cache line has been loaded into the CPU cache.</p>\n<h2>Required features</h2>\n<p>Why not use an open-source state-of-the-art hash table implementation such as\nSwiss tables? The answer is that we require some specific features, apart from\nthe basic operations like add, lookup, replace and delete:</p>\n<ul>\n<li><p>Incremental rehashing, so that when the hashtable is full, we don&#39;t freeze the\nserver while the table is being resized.</p>\n</li>\n<li><p>Scan, a way to iterate over the hash table even if the hash table is resized\nbetween the iterations. This is important to keep supporting the\n<a href=\"/commands/scan/\">SCAN</a> command.</p>\n</li>\n<li><p>Random element sampling, for commands like <a href=\"/commands/randomkey/\">RANDOMKEY</a>.</p>\n</li>\n</ul>\n<p>These aren&#39;t standard features, so we couldn&#39;t simply pick an off-the-shelf hash\ntable. We had to design one ourselves.</p>\n<h2>Design</h2>\n<p>In the new hash table designed for Valkey 8.1, the table consists of buckets of\n64 bytes, one cache line. Each bucket can store up to seven elements. Keys that\nmap to the same bucket are all stored in the same bucket. The bucket also\ncontains a metadata section, marked &quot;m&quot; in the figures. The bucket layout\nincluding the metadata section is explained in more detail below.</p>\n<p>We&#39;ve eliminated the <code>dictEntry</code> and instead embed key and value in the\n<code>serverObject</code>, along with other data for the key.</p>\n<p><img src=\"/content/blog/2025-03-28-new-hash-table/hashtable-structure.png\" alt=\"hashtable structure\" /></p>\n<p>Assuming the <code>hashtable</code> structure is already in the CPU cache, looking up\nkey-value entry now requires only two memory lookups: The bucket and the\n<code>serverObject</code>. If there is a hash collision, the object we&#39;re looking for is\nmost likely in the same bucket, so no extra memory access is required.</p>\n<p>If a bucket becomes full, the last element slot in the bucket is replaced by a\npointer to a child bucket. A child bucket has the same layout as a regular\nbucket, but it&#39;s a separate allocation. The lengths of these bucket chains are\nnot bounded, but long chains are very rare as long as keys are well distributed\nby the hashing function. Most of the keys are stored in top-level buckets.</p>\n<p><img src=\"/content/blog/2025-03-28-new-hash-table/hashtable-child-buckets.png\" alt=\"hashtable structure\" /></p>\n<p>The elements in the same bucket, or bucket chain, are stored without any\ninternal ordering. When inserting a new entry into the bucket, any of the free\nslots can be used.</p>\n<p>As mentioned earlier, each bucket also contains a metadata section. The bucket\nmetadata consists of eight bytes of which one bit indicates whether the bucket\nhas a child bucket or not. The next seven bits, one bit for each of the seven\nelement slots, indicates whether that slot is filled, i.e. whether it contains\nan element or not. The remaining seven bytes are used for storing a one byte\nsecondary hash for each of the entries stored in the bucket.</p>\n<p><img src=\"/content/blog/2025-03-28-new-hash-table/hash-bucket-structure.png\" alt=\"bucket structure\" /></p>\n<p>The secondary hash is made up of hash bits that are not used when looking up the\nbucket. Out of a 64 bits hash, we need not more than 56 bits for looking up the\nbucket and we use the remaining 8 bits as the secondary hash. These hash bits\nare used for quickly eliminating mismatching entries when looking up a key\nwithout comparing the keys. Comparing the keys of each entry in the bucket would\nrequire an extra memory access per entry. If the secondary hash mismatches the\nkey we&#39;re looking for, we can immediately skip that entry. The chance of a false\npositive, meaning an entry for which the secondary hash is matching although the\nentry doesn&#39;t match the key were looking for, is one in 256, so this eliminates\n99.6% of the false positives.</p>\n<h2>Results</h2>\n<p>By replacing the hash table with a different implementation, we&#39;ve managed to\nreduce the memory usage by roughly 20 bytes per key-value pair.</p>\n<p>The graph below shows the memory overhead for different value sizes. The\noverhead is the memory usage excluding the key and the value itself. Lower is\nbetter. (The zigzag pattern is because of unused memory resulting from the memory\nallocator&#39;s discrete allocation sizes.)</p>\n<p><img src=\"/content/blog/2025-03-28-new-hash-table/memory-usage.png\" alt=\"memory usage by version\" /></p>\n<p>For keys with an <a href=\"/commands/expire/\">expire time</a> (time-to-live, TTL) the memory\nusage is down even more, roughly 30 bytes per key-value pair.</p>\n<p><img src=\"/content/blog/2025-03-28-new-hash-table/memory-usage-with-expire.png\" alt=\"memory usage with expire\" /></p>\n<p>In some workloads, such as when storing very small objects and when pipelining\nis used extensively, the latency and CPU usage are also improved. In most cases\nthough this is negligible in practice. The key takeaway appears to be reduced\nmemory usage.</p>\n<h2>Hashes, sets and sorted sets</h2>\n<p>The nested data types Hashes, Sets and Sorted sets also make use of the new hash\ntable when they contain a sufficiently large number of elements. The memory\nusage is down by roughly 10-20 bytes per element for these types of keys.</p>\n<p>Special thanks to Rain Valentine for the graphs and for the help with\nintegrating this hash table into Valkey.</p>\n",
    "slug": "new-hash-table",
    "category": "news",
    "imageUrl": "/assets/media/blog/random-04.webp",
    "authorUsernames": [
      "zuiderkwast"
    ],
    "trending": false
  },
  {
    "title": "Introducing the Valkey Glide Go Client: Now in Public Preview!",
    "date": "2025-03-04T07:01:01.000Z",
    "excerpt": "Valkey Glide now supports GO. Read to learn more about the new client designed for performance and developer productivity",
    "content": "<p>Valkey-Glide is pleased to announce the public preview release of the GLIDE(General Language Independent Driver for the Enterprise) Go client. This release brings the power and reliability of Valkey to Go developers with an API designed for performance and developer productivity.</p>\n<p>Valkey GLIDE is a multi-language client for Valkey, designed for operational excellence and incorporating best practices refined through years of experience. GLIDE ensures a consistent and unified client experience across applications, regardless of the programming language.</p>\n<p>Currently, GLIDE supports Java, Node.js, and Python. This announcement introduces the Valkey GLIDE support for Go, expanding support to Go developers and providing new connectivity to Valkey servers, including both standalone and cluster deployments.</p>\n<h2>Why You Should Be Excited</h2>\n<p>The Go client extends Valkey GLIDE to the Go community, offering a robust, client that&#39;s built on the battle-tested Rust core. This client library is a thoughtfully designed experience for Go developers who need reliable, high-performance data access.</p>\n<h2>Key Features</h2>\n<h3>Advanced Cluster Topology Management</h3>\n<p>Connect to your Valkey cluster with minimal configuration. The client automatically detects the entire cluster topology and configures connection management based on industry best practices.</p>\n<pre><code class=\"language-go\">config := api.NewGlideClusterClientConfiguration().\n    WithAddress(&amp;api.NodeAddress{Host: &quot;localhost&quot;, Port: 6379})\n\nclient, err := api.NewGlideClusterClient(config)\n</code></pre>\n<p>The Go client provides advanced topology managements features such as:</p>\n<h4>Automatic Topology Discovery</h4>\n<p>GLIDE automatically discovers all cluster nodes from a single seed node, eliminating the need to manually configure every node address. The NodeAddress can be an IP address, hostname, or fully qualified domain name (FQDN).</p>\n<h4>Dynamic Topology Maintenance</h4>\n<p>Cluster topology can change over time as nodes are added, removed, or when slot ownership changes. GLIDE implements several mechanisms to maintain an accurate view of the cluster:</p>\n<ul>\n<li><strong>Proactive Topology Monitoring</strong>: GLIDE performs periodic background checks for cluster topology changes. This approach ensures a comprehensive and up-to-date view of the cluster, improving availability and reducing tail latency.</li>\n<li><strong>Consensus-Based Resolution</strong>: GLIDE queries multiple nodes for their topology view and selects the one with the highest agreement, reducing the risk of stale or incorrect mappings and ensuring a more accurate and up-to-date cluster view, improving the overall availability of the cluster.</li>\n<li><strong>Efficient Resource Management</strong>: GLIDE employs an efficient algorithm to compare node views and dynamically throttles client-management requests to prevent overloading Valkey servers, ensuring a balance between maintaining an up-to-date topology map and optimizing resource utilization.</li>\n</ul>\n<h3>Enhanced Connection Management</h3>\n<p>Connection management in distributed systems presents unique challenges that impact performance, reliability, and resource utilization. The Go client addresses these challenges with reliable solutions:</p>\n<h4>Proactive Reconnection</h4>\n<p>GLIDE implements a background monitoring system for connection states. By detecting disconnections and initiating reconnections preemptively, the client eliminates the reconnection latency typically experienced when a request discovers a broken connection.</p>\n<h4>Connection Storm Prevention</h4>\n<p>When network events occur, connection storms can overwhelm servers with simultaneous reconnection attempts. GLIDE mitigates this risk through backoff algorithm with jitter that distributes reconnection attempts over time, protecting servers from sudden connection surges.</p>\n<p>Robust connection handling with automatic reconnection strategies ensures your application remains resilient even during network instability:</p>\n<pre><code class=\"language-go\">// Configure a custom reconnection strategy with exponential backoff\nconfig := api.NewGlideClientConfiguration().\n    WithAddress(&amp;api.NodeAddress{Host: &quot;localhost&quot;, Port: 6379}).\n    WithReconnectStrategy(api.NewBackoffStrategy(\n        5, // Initial delay in milliseconds\n        10, // Maximum attempts\n        50 // Maximum delay in milliseconds\n    ))\n</code></pre>\n<h4>Multiplexed Connection</h4>\n<p>Rather than maintaining connection pools, GLIDE establishes a single multiplexed connection per cluster node. This architectural choice:</p>\n<ul>\n<li>Minimizes the total number of TCP connections to servers</li>\n<li>Reduces system call overhead</li>\n<li>Maintains high throughput through efficient connection pipelining</li>\n<li>Decreases server-side connection management burden</li>\n</ul>\n<h3>Built for Performance</h3>\n<p>The Go client is designed from the ground up with performance in mind while still being simple to use.\nThe Go client provides a synchronous API for simplicity and compatibility with existing Go key-value store clients. While each individual command is blocking (following the familiar patterns in the ecosystem), the client is fully thread-safe and designed for concurrent usage:</p>\n<pre><code class=\"language-go\">// Example of concurrent execution using goroutines\nfunc performConcurrentOperations(client *api.GlideClient) {\n    var wg sync.WaitGroup\n    \n    // Launch 10 concurrent operations\n    for i := 0; i &lt; 10; i++ {\n        wg.Add(1)\n        go func(idx int) {\n            defer wg.Done()\n            key := fmt.Sprintf(&quot;key:%d&quot;, idx)\n            value := fmt.Sprintf(&quot;value:%d&quot;, idx)\n            \n            // Each command blocks within its goroutine, but all 10 run concurrently\n            _, err := client.Set(key, value)\n            if err != nil {\n                fmt.Printf(&quot;Error setting %s: %v\\n&quot;, key, err)\n                return\n            }\n            \n            result, err := client.Get(key)\n            if err != nil {\n                fmt.Printf(&quot;Error getting %s: %v\\n&quot;, key, err)\n                return\n            }\n            \n            fmt.Printf(&quot;Result for %s: %s\\n&quot;, key, result)\n        }(i)\n    }\n    \n    wg.Wait()\n}\n</code></pre>\n<p>Under the hood, the client efficiently handles these concurrent requests by:</p>\n<ol>\n<li>Using a single multiplexed connection per node to pipeline concurrent commands, minimizing socket overhead and system resources</li>\n<li>Implementing thread-safe command execution</li>\n<li>Efficiently routing concurrent commands to the appropriate server nodes</li>\n</ol>\n<p>While the current API is synchronous, the implementation is specifically optimized for concurrent usage through Go&#39;s native goroutines. We would love feedback about whether to add async/channel-based APIs in future releases.</p>\n<h2>Getting Started</h2>\n<p>You can add Valkey GLIDE to your project with the following two commands:</p>\n<pre><code class=\"language-bash\">go get github.com/valkey-io/valkey-glide/go\ngo mod tidy\n</code></pre>\n<p>Then, you can get started connecting to a Valkey standalone server, running locally on port 6379, with the following sample applications:</p>\n<pre><code class=\"language-go\">package main\n\nimport (\n    &quot;fmt&quot;\n    &quot;github.com/valkey-io/valkey-glide/go/api&quot;\n)\n\nfunc main() {\n    // Connect to a standalone Valkey server\n    config := api.NewGlideClientConfiguration().\n        WithAddress(&amp;api.NodeAddress{Host: &quot;localhost&quot;, Port: 6379})\n    \n    client, err := api.NewGlideClient(config)\n    if err != nil {\n        fmt.Println(&quot;Error:&quot;, err)\n        return\n    }\n    defer client.Close()\n    \n    // Test the connection\n    result, err := client.Ping()\n    if err != nil {\n        fmt.Println(&quot;Error:&quot;, err)\n        return\n    }\n    fmt.Println(result) // PONG\n    \n    // Store and retrieve a value\n    client.Set(&quot;hello&quot;, &quot;valkey&quot;)\n    value, _ := client.Get(&quot;hello&quot;)\n    fmt.Println(value) // valkey\n}\n</code></pre>\n<h3>Cluster Mode Connection Setup</h3>\n<p>Need to work with a Valkey cluster?</p>\n<p>Just as easy! The Go client automatically discovers your entire cluster topology from a single seed node. The following sample shows how to connect to a Valkey cluster through a node running locally on port 7001:</p>\n<pre><code class=\"language-go\">package main\n\nimport (\n    &quot;fmt&quot;\n    &quot;github.com/valkey-io/valkey-glide/go/api&quot;\n)\n\nfunc main() {\n    // Specify the address of any single node in your cluster\n    // This example connects to a local cluster node on port 7001\n    host := &quot;localhost&quot;\n    port := 7001\n    \n    // Connect to a Valkey cluster through any node\n    config := api.NewGlideClusterClientConfiguration().\n        WithAddress(&amp;api.NodeAddress{Host: host, Port: port})\n    \n    client, err := api.NewGlideClusterClient(config)\n    if err != nil {\n        fmt.Println(&quot;There was an error: &quot;, err)\n        return\n    }\n    \n    res, err := client.Ping()\n    if err != nil {\n        fmt.Println(&quot;There was an error: &quot;, err)\n        return\n    }\n    fmt.Println(res) // PONG\n    client.Close()\n}\n</code></pre>\n<h2>Advanced Configuration Options</h2>\n<h3>Read Strategies for Optimized Performance</h3>\n<p>Balance consistency and throughput with flexible read strategies:</p>\n<pre><code class=\"language-go\">// Configure to prefer replicas for read operations\nconfig := api.NewGlideClusterClientConfiguration().\n    WithAddress(&amp;api.NodeAddress{Host: &quot;cluster.example.com&quot;, Port: 6379}).\n    WithReadFrom(api.PreferReplica)\n\nclient, err := api.NewGlideClusterClient(config)\n\n// Write to primary\nclient.Set(&quot;key1&quot;, &quot;value1&quot;)\n\n// Automatically reads from a replica (round-robin)\nresult, err := client.Get(&quot;key1&quot;)\n</code></pre>\n<p>Available strategies:</p>\n<ul>\n<li><strong>PRIMARY</strong>: Always read from primary nodes for the freshest data</li>\n<li><strong>PREFER_REPLICA</strong>: Distribute reads across replicas in round-robin fashion, falling back to primary when needed</li>\n</ul>\n<p>Planned for future release:</p>\n<ul>\n<li><strong>AZ_AFFINITY</strong>: (Coming soon) Prefer replicas in the same availability zone as the client</li>\n</ul>\n<h3>Authentication and TLS</h3>\n<p>Secure your connections with built-in authentication and TLS support:</p>\n<pre><code class=\"language-go\">// Configure with authentication\nconfig := api.NewGlideClientConfiguration().\n    WithAddress(&amp;api.NodeAddress{Host: &quot;localhost&quot;, Port: 6379}).\n    WithCredentials(api.NewServerCredentials(&quot;username&quot;, &quot;password&quot;)).\n    WithUseTLS(true) // Enable TLS for encrypted connections\n</code></pre>\n<h3>Request Timeout and Handling</h3>\n<p>Fine-tune timeout settings for different workloads:</p>\n<pre><code class=\"language-go\">// Set a longer timeout for operations that may take more time\nconfig := api.NewGlideClientConfiguration().\n    WithAddress(&amp;api.NodeAddress{Host: &quot;localhost&quot;, Port: 6379}).\n    WithRequestTimeout(500) // 500ms timeout\n</code></pre>\n<h2>Behind the Scenes: Technical Architecture</h2>\n<p>The Valkey GLIDE Go client is built on top of the Valkey GLIDE core. The core framework is written in Rust (lib.rs), which exposes public functions. These functions are converted to a C header file using Cbindgen. The Go client then uses CGO to call these C functions, providing Go developers with an idiomatic interface while leveraging Rust&#39;s performance advantages. This architecture ensures consistent behavior across all Valkey GLIDE language implementations (Java, Python, Node.js, and Go) while maintaining performance and reliability.</p>\n<h3>Component details</h3>\n<pre><code class=\"language-text\">+------------+      +------+      +------------+      +------------+      +------------+\n|            |      |      |      |            |      |            |      |            |\n|    Go      |-----&gt;|      |-----&gt;|  C Header  |-----&gt;|    Rust    |-----&gt;|   Valkey   |\n|  Client    |      |  CGO |      |  cbindgen  |      |    Core    |      |   Server   |\n|            |&lt;-----|      |&lt;-----|            |&lt;-----|            |&lt;-----|            |\n|            |      |      |      |            |      |            |      |            |\n+------------+      +------+      +------------+      +------------+      +------------+\n</code></pre>\n<ul>\n<li><strong>Go Client</strong>: The language-specific interface for Go developers</li>\n<li><strong>CGO</strong>: Allows Go code to call C functions</li>\n<li><strong>Cbindgen</strong>: Automates the generation of C header files from Rust public APIs</li>\n<li><strong>Rust Core</strong>: High-performance framework that connects to and communicates with Valkey servers</li>\n<li><strong>Rust FFI Library</strong>: Enables cross-language function calls between Rust and other languages</li>\n</ul>\n<h2>Join the Journey</h2>\n<p>This public preview is just the beginning. We&#39;re actively developing and enhancing the Go wrapper, and we&#39;d love your feedback and contributions. Try it out in your projects, share your experiences, and help us make it even better!\nYou can join our development journey by:</p>\n<ul>\n<li>Submitting issues or feature requests on our <a href=\"https://github.com/valkey-io/valkey-glide/issues\">GitHub Issues page</a></li>\n<li>Joining discussions in our <a href=\"https://github.com/valkey-io/valkey-glide/discussions\">GitHub Discussions forum</a></li>\n</ul>\n<h2>Looking Forward</h2>\n<p>As we move toward general availability, we&#39;ll be expanding command support, enhancing performance, and adding even more features to make the Valkey GLIDE Go client a great choice for Go developers.</p>\n<p>Checkout our <a href=\"https://github.com/valkey-io/valkey-glide/tree/main/go\">Valkey GLIDE go client</a> for the source code.\nFor implementation examples, please refer to the <a href=\"https://github.com/valkey-io/valkey-glide/blob/main/go/README\">README of the Go examples</a> for instructions on running the Standalone and Cluster examples.</p>\n<p>For a complete reference of all available commands and their parameters, explore the <a href=\"https://pkg.go.dev/github.com/valkey-io/valkey-glide/go/api\">Go API documentation on pkg.go.dev</a>, which provides detailed information on method signatures, parameters, and return types.</p>\n<h2>Contributors</h2>\n<p>A huge thank you to all the contributors who have made this possible - your dedication and expertise have created something truly special for the Go community.</p>\n<p><a href=\"https://github.com/janhavigupta007\">Janhavi Gupta</a> (Google Cloud Platform)</p>\n<p><a href=\"https://github.com/niharikabhavaraju\">Niharika Bhavaraju</a> (Google Cloud Platform)</p>\n<p><a href=\"https://github.com/EdricCua\">Edric Cuartero</a> (Google Cloud Platform)</p>\n<p><a href=\"https://github.com/omangesg\">Omkar Mestry</a> (Google Cloud Platform)</p>\n<p><a href=\"https://github.com/Yury-Fridlyand\">Yury Fridlyand</a> (Improving)</p>\n<p><a href=\"https://github.com/prateek-kumar-improving\">Prateek Kumar</a> (Improving)</p>\n<p>Kudos to <a href=\"https://github.com/aaron-congo\">Aaron Congo</a> who created the backbone of the client 🚀 and to <a href=\"https://github.com/umit\">Umit Unal</a>, <a href=\"https://github.com/MikeMwita\">Michael</a> for their contributions!</p>\n",
    "slug": "2025-03-4-go-client-in-public-preview",
    "category": "news",
    "imageUrl": "/assets/media/blog/random-06.webp",
    "authorUsernames": [
      "niharikabhavaraju"
    ],
    "trending": true
  },
  {
    "title": "Reducing application latency and lowering Cloud bill by setting up your client library",
    "date": "2025-01-08T07:01:01.000Z",
    "excerpt": "By implementing AZ affinity routing in Valkey and using GLIDE, you can achieve lower latency and cost savings by routing requests to replicas in the same AZ as the client.",
    "content": "<p>How can adjusting your client library help you reduce Cloud costs and improve latency?</p>\n<p>In this post, we dive into <strong>Availability Zone (AZ) affinity routing</strong> mechanics, showing how it optimizes your application&#39;s performance and cost using <strong>Valkey GLIDE (General Language Independent Driver for the Enterprise)</strong>. We also guide you through how to configure GLIDE to benefit from other key features. </p>\n<p>GLIDE is an official open-source Valkey client library. GLIDE is designed for reliability, optimized performance, and high-availability, for Valkey and Redis-based applications. GLIDE is a multi-language client that supports Java, Python, and Node.js, with further languages in development. GLIDE recently added support for a key feature AZ affinity routing, which enables Valkey-based applications to direct calls specifically to server nodes in the same AZ as the client. This minimizes cross-AZ traffic, reduces latency, and lowers cloud expenses.</p>\n<p>Before we explore AZ affinity routing, let’s understand what availability zones are, how different <strong>read strategies</strong> work and how they impact your application.</p>\n<h2>Choosing the Right Read Strategy for Your Application</h2>\n<p>Distributed applications rely on scalability and resilience, often achieved by techniques like caching, sharding, and high availability. Valkey enhances these systems by acting as a robust caching layer, reducing database load and accelerating read operations. Its sharding capabilities distribute data across multiple nodes, ensuring efficient storage and access patterns, while its high availability features safeguard uptime by replicating data across the primary and replica nodes. This combination enables distributed applications to handle high traffic and recover quickly from failures, ensuring consistent performance.</p>\n<p>In Valkey-based applications, selecting the right read strategy is required for optimizing performance and cost. Read strategies determine how read-only commands are routed, balancing factors like data freshness, latency, and throughput. \nUnderstanding the infrastructure that supports these strategies is key to leveraging them effectively.</p>\n<p><strong>Availability Zones</strong> are isolated locations within Cloud regions that provide redundancy and fault tolerance. They are physically separated but connected through low-latency networks. Major cloud providers like AWS, Oracle and GCP implement the concept of AZs. However, using resources across different AZs can incur increased latency and cost.\nGLIDE takes advantage of this infrastructure by routing reads to replica nodes within the same AZ, enabling faster responses and improved user experience. \nThis is particularly advantageous for applications that prioritize read throughput and can tolerate slightly stale data. For instance, websites with personalized recommendation engines rely on displaying content quickly to users rather than ensuring every update is perfectly synchronized.\nAdditionally, one of the most common use cases for caching is to store database query results, allowing applications to trade off absolute freshness for better performance, scalability, and cost-effectiveness. The read-from-replica strategies introduces minimal additional staleness, making it an efficient choice for such scenarios.\nGLIDE provides flexible options tailored to your application’s needs:</p>\n<ul>\n<li><code>PRIMARY</code>: Always read from the primary to ensure the freshness of data.</li>\n<li><code>PREFER_REPLICA</code>: Distribute requests among all replicas in a round-robin manner. If no replica is available, fallback to the primary.</li>\n<li><code>AZ_AFFINITY</code>: Prioritize replicas in the same AZ as the client. If no replicas are available in the zone, fallback to other replicas or the primary if needed.</li>\n</ul>\n<p>In Valkey 8,  <code>availability-zone</code> configuration was introduced, allowing clients to specify the AZ for each Valkey server. GLIDE leverages this new configuration to empower its users with the ability to use AZ Affinity routing. At the time of writing, GLIDE is the only Valkey client library supporting the AZ Affinity strategy, offering a unique advantage.</p>\n<h2>AZ Affinity routing advantages</h2>\n<ol>\n<li><p><strong>Reduce Data Transfer Costs</strong> Cross-zone data transfer often incurs additional charges in Cloud environments. By ensuring operations are directed to nodes within the same AZ, you can minimize or eliminate these costs.</p>\n<p> <strong>Example:</strong> An application in AWS with a Valkey cluster of 2 shards, each with 1 primary and 2 replicas, the instance type is m7g.xlarge. The cluster processes 250MB of data per second and to simplify the example 100% of the traffic is read operation. 50% of this traffic crosses AZs at a cost of $0.01 per GB, the monthly cross-AZ data transfer cost would be approximately $3,285. In addition the cost of the cluster is $0.252 per hour per node. Total of $1,088 per month. By implementing AZ affinity routing, you can reduce the total cost from $4,373 to $1,088, as all traffic remains within the same AZ.</p>\n</li>\n<li><p><strong>Minimize Latency</strong> Distance between AZs within the same region— for example, in AWS, is typically up to 60 miles (100 kilometers)—adds extra roundtrip latency, usually in the range of 500µs to 1000µs. By ensuring requests remain within the same AZ, you can reduce latency and improve the responsiveness of your application.</p>\n<p> <strong>Example:</strong>\n Consider a cluster with three nodes, one primary and two replicas. Each node is located in a different availability zone. The client located in az-2 along with replica-1. </p>\n<p> <strong>With <code>PREFER_REPLICA</code> strategy</strong>:\n In this case, the client will read commands from any replica that is available. It may be located in a different AZ as shown below, and the average latency is, for example, 800 microseconds.</p>\n<p> <img src=\"/content/blog//assets/media/pictures/PREFER_REPLICA_strategy.png\" alt=\"PREFER_REPLICA Read strategy latency example\" /></p>\n<p> <strong>With <code>AZ_AFFINITY</code> strategy</strong>:\n In this case, the client will read commands from a replica in the same client&#39;s AZ and the average latency is, for example, about 300 microseconds.</p>\n<p> <img src=\"/content/blog//assets/media/pictures/AZ_AFFINITY_strategy.png\" alt=\"AZ_AFFINITY Read strategy latency example\" /></p>\n</li>\n</ol>\n<h2>Configuring AZ Affinity Connections with GLIDE</h2>\n<p>Setting up AZ affinity routing in GLIDE is simple, allowing you to leverage its full potential with just a few configuration steps. Let’s walk through the steps to enable this feature in your application.</p>\n<h3>Steps to Set Up AZ Affinity Routing in GLIDE</h3>\n<ol>\n<li><p>Configure Valkey nodes availability zone - \n Assign each Valkey node to a specific AZ based on its physical or virtual location within the Cloud provider&#39;s region. \n The initial configuration must to be done with a separate management client on node initialization, as the clients gets the info from the replicas on the first reconnect. \n In some managed services like Amazon ElastiCache, this mapping is configured automatically and this step is not required. </p>\n<p> For each node, run the following command and change the AZ and routing address as appropriate:</p>\n<p> <strong>Python:</strong></p>\n<pre><code class=\"language-python\">client.config_set({&quot;availability-zone&quot;: az}, \n                    route=ByAddressRoute(host=&quot;address.example.com&quot;, port=6379))\n</code></pre>\n<p> <strong>Java:</strong></p>\n<pre><code class=\"language-Java\">client.configSet(Map.of(&quot;availability-zone&quot;, az), new ByAddressRoute(&quot;address.example.com&quot;, 6379))\n</code></pre>\n<p> <strong>Node.js:</strong></p>\n<pre><code class=\"language-javascript\">client.configSet({&quot;availability-zone&quot;: az}, { route: {type: &quot;routeByAddress&quot;, host:&quot;address.example.com&quot;, port:6379}})\n</code></pre>\n</li>\n<li><p>Configure GLIDE with AZ-Specific Targeting - \n Here are Python, Java, and Node.JS examples showing how to set up an AZ affinity client that directs calls to nodes in the same AZ as the client.</p>\n<p> <strong>Python:</strong></p>\n<pre><code class=\"language-python\">from glide import (\n    GlideClient,\n    GlideClientConfiguration,\n    NodeAddress,\n    ReadFrom\n)\n\n# Determine the client&#39;s AZ (this could be fetched from the cloud provider&#39;s metadata service)\nclient_az = &#39;us-east-1a&#39;\n\n# Initialize Valkey client with preference for the client&#39;s AZ\naddresses = [NodeAddress(host=&quot;address.example.com&quot;, port=6379)]\nclient_config = GlideClusterClientConfiguration(addresses, read_from=ReadFrom.AZ_AFFINITY, client_az=client_az)\nclient = await GlideClusterClient.create(client_config)\n\n# Write operation (route to the primary&#39;s slot owner)\nclient.set(&quot;key1&quot;, &quot;val1&quot;)\n\n# Get will read from one of the replicas in the same client&#39;s availability zone if one exits.\nvalue = client.get(&quot;key1&quot;)\n</code></pre>\n<p> <strong>Java:</strong></p>\n<pre><code class=\"language-Java\">// Initialize Valkey client with preference for the client&#39;s AZ\nGlideClusterClientConfiguration config = GlideClusterClientConfiguration.builder()\n    .address(NodeAddress.builder()\n        .host(&quot;address.example.com&quot;)\n        .port(6379)\n        .build())\n    .readFrom(ReadFrom.AZ_AFFINITY)\n    .clientAZ(&quot;us-east-1a&quot;)\n    .build()\nGlideClusterClient client = GlideClusterClient.createClient(config).get();\n\n// Write operation (route to the primary&#39;s slot owner)\nclient.set(&quot;key1&quot;, &quot;val1&quot;).get();\n\n// Get will read from one of the replicas in the same client&#39;s availability zone if one exits.\nclient.get(&quot;key1&quot;).get();\n</code></pre>\n<p> <strong>Node.js:</strong></p>\n<pre><code class=\"language-javascript\">import GlideClusterClient from &quot;@valkey/valkey-glide&quot;;\n\nconst addresses = [\n    {\n        host: &quot;address.example.com&quot;,\n        port: 6379\n    }\n];\n\n// Initialize Valkey client with preference for the client&#39;s AZ\nconst client = await GlideClusterClient.createClient({\n    addresses: addresses,\n    readFrom: &quot;AZAffinity&quot; as ReadFrom,\n    clientAz: &quot;us-east-1a&quot;,\n});\n// Write operation (route to the primary&#39;s slot owner)\nawait client.set(&quot;key1&quot;, &quot;val1&quot;);\n// Get will read from one of the replicas in the same client&#39;s availability zone if one exits.\nawait client.get(&quot;key1&quot;);\n</code></pre>\n</li>\n</ol>\n<h2>Conclusion</h2>\n<p>By implementing AZ affinity routing in Valkey and using GLIDE, you can achieve lower latency and cost savings by routing requests to replicas in the same AZ as the client.</p>\n<h3>Further Reading</h3>\n<ul>\n<li><a href=\"https://github.com/valkey-io/valkey-glide\">Valkey GLIDE GitHub Repository</a></li>\n<li><a href=\"https://valkey.io/\">Valkey Documentation</a></li>\n<li><a href=\"https://github.com/valkey-io/valkey-glide/wiki/Python-wrapper#read-strategy\">Valkey GLIDE read strategy documentation in Python</a> </li>\n<li><a href=\"https://github.com/valkey-io/valkey-glide/wiki/Java-Wrapper#read-strategy\">Valkey GLIDE read strategy documentation in Java</a></li>\n<li><a href=\"https://github.com/valkey-io/valkey-glide/wiki/NodeJS-wrapper#read-strategy\">Valkey GLIDE read strategy documentation in NodeJS</a></li>\n</ul>\n",
    "slug": "az-affinity-strategy",
    "category": "news",
    "imageUrl": "/assets/media/blog/default.webp",
    "authorUsernames": [
      "asafporatstoler",
      "adarovadya"
    ],
    "trending": true
  },
  {
    "title": "2024: The Year of Valkey",
    "date": "2024-12-20T07:01:01.000Z",
    "excerpt": "The end of the calendar year is a great time to reflect, but for Valkey this particular year-end holds special meaning.",
    "content": "<p>The end of the calendar year is a great time to reflect, but for Valkey this particular year-end holds special meaning.\nThink about it: this time in 2023, no one had ever heard the name “Valkey” because, well, it didn’t exist.\nThis seems nearly unbelievable given how much has changed in only a few short months.</p>\n<p>Now, at the end of 2024, Valkey has had both a minor and major release as well as a few patches.\nValkey 7.2 primarily introduced the project and new name while carrying over the feature set and performance from before the fork.\nValkey 8.0 made substantial internal changes, bringing higher performance through multi-threaded I/O, better memory efficiency from a rewritten main dictionary, more granular visibility into performance and resource usage, and enhanced reliability in replication and slot migration.\nIn 2025, the project is looking toward a future with new functionality and a whole boatload of optimizations in both performance and efficiency.</p>\n<h2>Beginnings</h2>\n<p><a href=\"https://youtu.be/74Svvu37I_8?si=onLIvlu3X_ncKdh2&t=3055\"><img src=\"/content/blog/2024-12-20-2024-year-of-valkey/images/ossna-thumb.jpg\" alt=\"YouTube video thumbnail of open source summit north america\" /></a></p>\n<p>Getting started with Valkey has changed substantially over the course of the year.\nWhen Valkey first launched at Open Source Summit North America, I recall excitedly telling people that you could build from source, get the binary from the website or even use a container.\nNow, you can get Valkey directly from the <a href=\"https://repology.org/project/valkey/versions\">package manager in most Linux distributions</a> (and more on the way in 2025).\nThere are multiple options for containerization and operators to fit your needs.\nAnd, for those who want to let <em>others</em> run Valkey, it is also available as a service on Aiven, AWS, Google Cloud Platform, NetApp Instaclustr, UpCloud, and several more.</p>\n<p>The buzz around Valkey caused no shortage of coverage.\nOne theme that the initial Valkey coverage focused on was the speed that everything happened: only 8 days after the license change, the project had a release, was a Linux foundation project with supporters from a variety of companies around the industry:</p>\n<blockquote>\n<p><em>An eight day timeline for a project of this size and scope is shocking.\nAn eight day timeline to announce a project of this scope that includes names like AWS, Google and Oracle of all vendors is an event without any obvious precedent.\nEven setting the requisite legal approvals aside, naming and branding take time – as they did in this case, apparently.</em></p>\n</blockquote>\n<blockquote>\n<p><a href=\"https://redmonk.com/sogrady/2024/07/16/post-valkey-world/\">Stephen O’Grady</a></p>\n</blockquote>\n<p>Over the course of the year, the coverage shifted focus from establishment to velocity:</p>\n<blockquote>\n<p><em>As per GitHub and as of this writing, it’s got roughly 10 times as many contributors and is hundreds of commits ahead of Redis.\nIt’s like Redis, but with more caffeine, a bigger dev team, and a community that’s suddenly not beholden to keeping requested features stuffed behind a paywall.</em></p>\n</blockquote>\n<blockquote>\n<p><a href=\"https://www.lastweekinaws.com/blog/aws-valkey-play-when-a-fork-becomes-a-price-cut/\">Corey Quinn</a></p>\n</blockquote>\n<h2>Stories</h2>\n<p>Beyond seeing this coverage, less than six months after the establishment of the project a report indicated that <a href=\"https://thenewstack.io/redis-users-want-a-change/\">63% of respondents were already familiar with Valkey</a>, so it’s no surprise that there are countless folks already using it.\nThese specific stories being told in public are a valuable data point for the next wave of migrations.\nAt Open Source Summit Hong Kong in September, <a href=\"https://www.facesofopensource.com/dirk-hohndel/\">Dirk Hohndel</a> from Verizon talks about migrating his own app over to Valkey 8.0 release candidate and seeing a 3x performance increase.</p>\n<p><a href=\"https://www.youtube.com/watch?v=Qp74Nn-d5a8\"><img src=\"/content/blog/2024-12-20-2024-year-of-valkey/images/osshk-dirk.jpg\" alt=\"YouTube video thumbnail of open source summit hong kong\" /></a></p>\n<p>On <a href=\"https://aws.amazon.com/developer/community/heroes/marcin-sodkiewicz/\">Marcin Sodkiewicz</a>’s blog he talks a <a href=\"https://sodkiewiczm.medium.com/elasticache-serverless-valkey-review-1e3329cfbfa0\">little bit about his journey moving to Valkey</a>.\nThe post is a lot of math about the saving he’s achieving due to the service he’s using having a lower unit cost\nWhat’s perhaps most interesting in this account of migrating to Valkey is what <em>isn’t</em> said.\nNot once did Marcin talk about changing code, libraries, or commands: it was a “no-brainer” because he just had to update some infrastructure-as-code configuration to move to Valkey.</p>\n<p>Given the <a href=\"https://www.linuxfoundation.org/blog/iwb-2024-state-of-open-source-financial-services\">strategic relevance of open source software in the financial services industry</a>, seeing organizations come forward with strong supporting statements about their migration to Valkey in context.\nKailash Nadh, CTO of <a href=\"http://zerodha.com/\">Zerodha</a>, an <a href=\"https://en.wikipedia.org/wiki/Zerodha\">Indian online brokerage and financial services company</a> provided the following quote:</p>\n<blockquote>\n<p><em>We recently adopted Valkey following the unexpected changes with Redis.\nValkey is a crucial project for the ecosystem, and we’re closely monitoring its progress with the ultimate aim of fully migrating to it.\nWe are excited to support the project!</em></p>\n</blockquote>\n<p>Similarly, Linux distributions have <a href=\"https://repology.org/project/valkey/versions\">widely adopted Valkey in their package managers</a>.\nThe latest version of Fedora, version 41, goes beyond just packaging Valkey: <a href=\"https://fedoraproject.org/wiki/Changes/Replace_Redis_With_Valkey#Upgrade/compatibility_impact\">it obsoletes Redis in lieu of Valkey</a>.\nFunctionally, this means that if you upgraded to Fedora 41 and consumed Redis packaged by Fedora, you were automatically migrated to Valkey and all future attempts to install Redis from the package manager gives you Valkey.</p>\n<p><a href=\"https://almalinux.org/\">AlmaLinux, “the forever-free, Enterprise OS”</a> provides Valkey to it’s users but also is a user of Valkey in their mirroring system.\nIn an interview with Jonathan Wright, Lead for the infrastructure special interest group at AlmaLinux, he described how the mirror list system within AlmaLinux handles “millions and millions of requests per day” for systems requesting package updates.\nThe information is not simple: geolocation, subnet, and ASN matching means that querying a relational database directly was not an option: it would just be too inefficient and too slow.\nThe system, which originally ran on Redis, takes the total request time from “2-3 seconds per request” down to just 50 milliseconds.\nOn Alma’s migration to Valkey he said:</p>\n<blockquote>\n<p><em>...it was seamless ... you can yank out Redis, drop in Valkey and just keep on running, you don’t have to change anything.</em></p>\n</blockquote>\n<h2>Forward to 2025</h2>\n<p>Valkey has grown immensely in 2024.\n2025 will definitely hold new challenges but it’s sure to carry forward the growth and excitement about the project while still delivering performance, efficiency, and drama-free installation &amp; migrations to users.</p>\n",
    "slug": "2024-year-of-valkey",
    "category": "news",
    "imageUrl": "/assets/media/blog/random-01.webp",
    "authorUsernames": [
      "kyledvs"
    ],
    "trending": false
  },
  {
    "title": "Pushing the limits of Valkey on a Raspberry Pi",
    "date": "2024-11-21T07:01:01.000Z",
    "excerpt": "While most people won't go to production on a Raspberry Pi, we'll cover how to thoroughly performance test Valkey to understand how it works in production.",
    "content": "<p>While doing extensive performance testing on a Raspberry Pi is silly, it&#39;s made me realize the complexity of performance testing. For example, in some of the tests below I ended up managing to use all of the resources of the Raspberry Pi and achieved terrible performance. Every application has different performance characteristics so we&#39;ll walk through what factors to consider when it comes to deploying Valkey. </p>\n<h2>The test environment</h2>\n<p><img src=\"/content/blog/2024-11-21-testing-the-limits/images/cm4.png\" alt=\"Picture of the Compute Module 4 credit Raspberry Pi Ltd (CC BY-SA)\" /></p>\n<p>For hardware we are going to be using a Raspberry Pi <a href=\"https://www.raspberrypi.com/products/compute-module-4/?variant=raspberry-pi-cm4001000\">Compute Module 4 (CM4)</a>. It&#39;s a single board computer (SBC) that comes with a tiny 1.5Ghz 4-core Broadcomm CPU and 8GB of system memory. This is hardly the first device someone would pick when deciding on a production system. Using the CM4 makes it easy to showcase how to optimize Valkey depending on your different hardware constraints.</p>\n<p>Our operating system will be a 64-bit Debian based operating system (OS) called <a href=\"https://www.raspbian.org/\">Rasbian</a>. This distribution is specifically modified to perform well on the CM4. Valkey will run in a docker container orchestrated with docker compose. I like deploying in containers as it simplifies operations. If you&#39;d like to follow along here is <a href=\"https://docs.docker.com/engine/install/debian/\">a guide for installing Docker</a>. Make sure to continue to the <a href=\"https://docs.docker.com/engine/install/linux-postinstall/\">second page of the installation process</a> as well. It&#39;s easy to miss and skipping it could make it harder to follow along. </p>\n<p>We&#39;ll be using two CM4s for testing. The first will host Valkey and the second will host the benchmarking software. This setup probably better reflects how most people will run in production. Benchmarking is being done with redis-benchmark because it can be installed with <code>sudo apt install redis-tools</code>. Valkey does have its own benchmark utility that comes installed with Valkey. To use valkey-benchmark instead you would need to install Valkey on the benchmarking server or spin up a container and connect into it. Functionally, they both operate nearly the same as of the writing of this article. </p>\n<p><img src=\"/content/blog/2024-11-21-testing-the-limits/images/test_setup.png\" alt=\"Test architecture showing two nodes: a Benchmark server with the ip of 10.0.11.221 with an arrow pointing to a Valkey server with an ip of 10.0.1.136\" /></p>\n<h2>Setting up our environment</h2>\n<p>Below is a straightforward docker compose file that will start a single Valkey container. This container will bind Valkey to port 6379 on the host device. This means that is exposed to anyone with access to your network! This is important for us to access it from the benchmarking server. </p>\n<pre><code class=\"language-yaml\"># valkey.yaml\nservices:\n  valkey-1:\n    image: valkey/valkey:latest\n    hostname: valkey1\n    command: valkey-server --port 6379 --requirepass ${VALKEY_PASSWORD} --io-threads ${IO_THREADS} --save &quot;&quot;\n    volumes:\n      - ./data:/data\n    network_mode: host\n\nvolumes:\n  data:\n    driver: local\n</code></pre>\n<p>Since we are exposing this to our internal network we will create a password for our default user. I used <code>head -16 /dev/urandom | openssl sha1</code> to generate a random password. Because of how fast Valkey can process requests a brute force attack could try hundreds of thousands of passwords per second. After generating that password, I put it in a <code>.env</code> file in the same directory as our docker compose. </p>\n<pre><code class=\"language-bash\">#.env\nVALKEY_PASSWORD=e41fb9818502071d592b36b99f63003019861dad\nNODE_IP=&lt;VALKEY SERVER IP&gt;\nIO_THREADS=1\n</code></pre>\n<p>Now by running <code>docker compose -f valkey.yaml up -d</code> Valkey server will start with password we set! </p>\n<h2>Baseline test</h2>\n<p>Now we are ready to do some baseline testing. We will log into the benchmarking server. If you haven&#39;t installed redis-benchmark yet you can do so with <code>sudo apt install redis-tools</code>.  </p>\n<pre><code class=\"language-bash\">redis-benchmark -n 1000000 -t set,get -P 16 -q -a &lt;PASSWORD FROM .env&gt; --threads 5 -h 10.0.1.136\n</code></pre>\n<p>Test breakdown:</p>\n<ul>\n<li><code>-n</code> - this will run 1,000,000 operations using the commands in -t</li>\n<li><code>-t</code> - will run the set and get tests</li>\n<li><code>-P</code> - this specifies that we would like the tests to use 16 pipelines (send 16 operations per request). </li>\n<li><code>-q</code> - silences the output to show only the final results</li>\n<li><code>-a</code> - use the specified password</li>\n<li><code>-h</code> - run the test against the specified host</li>\n<li><code>--threads</code> - how many threads to generate test data from</li>\n</ul>\n<p>Honestly, I was astonished by the first set of results I got. Sure, I expected Valkey to be fast but this speed from a single board computer?? ON A SINGLE THREAD?? It&#39;s amazing. </p>\n<pre><code class=\"language-bash\">redis-benchmark -n 1000000 -t set,get -P 16 -q -a e41fb9818502071d592b36b99f63003019861dad --threads 5 -h 10.0.1.136\nSET: 173040.33 requests per second, p50=4.503 msec                    \nGET: 307031.00 requests per second, p50=2.455 msec\n</code></pre>\n<p>Between the two tests we averaged 240,000 requests per second.  </p>\n<h2>Raising the CPU clock speed</h2>\n<p>Since Valkey is a single threaded application, it makes sense that higher clock speeds would lead to more performance. I don&#39;t expect most people will overclock their servers in production. Different servers may be available with different CPU clock speeds. </p>\n<p><strong>Note:</strong> Clock speeds generally are only comparable between CPU&#39;s with a similar architecture. For example, you could reasonably compare clock speeds between an 12th generation Intel i5 and a 12th generation Intel i7. If the 12th gen i7 had a max clock speed of 5Ghz that doesn&#39;t necessarily mean it will be slower than a AMD Ryzen 9 9900X clocked at 5.6Ghz. </p>\n<p>If you&#39;re following along on a Pi of your own I&#39;ve outlined the steps to overclock your CM4 below. Otherwise you can skip to the results section below.</p>\n<p><strong>Warning</strong> Just a reminder overclocking your device can cause damage to your device. Please use caution and do your own research for settings that are safe. </p>\n<ol>\n<li>Open the below file<ul>\n<li><code>sudo nano /boot/firmware/config.txt</code></li>\n</ul>\n</li>\n<li>At the end of the file add this section below the <code>[all]</code> tag<pre><code>[all]\nover_voltage=8\narm_freq=2200\n</code></pre>\n</li>\n<li>Restart the Pi and log back in <code>sudo restart now</code></li>\n</ol>\n<p>We&#39;ve just increased the speed of the Pi by 47% by raising the clock speed from 1.5Ghz to 2.2Ghz. Now lets re-run our test and see how things look!</p>\n<pre><code class=\"language-bash\">redis-benchmark -n 1000000 -t set,get -P 16 -q -a e41fb9818502071d592b36b99f63003019861dad --threads 5 -h 10.0.1.136\nSET: 394368.41 requests per second, p50=1.223 msec                    \nGET: 438058.53 requests per second, p50=1.135 msec \n</code></pre>\n<p>We&#39;re up to 416,000 requests per second (reminder this is the average between the two operations). The mathmaticians out there might notice that this speed up is a lot more than the expected 47% increase. It&#39;s a 73% increase in requests per second. What&#39;s happening?!</p>\n<h2>Adding IO Threading</h2>\n<p>With all these gains I was super excited to try the new <a href=\"https://valkey.io/blog/unlock-one-million-rps/\">IO threading available</a> in Valkey 8. First we will take down the previous running docker instance with <code>docker compose -f valkey.yaml down</code>. Then we will modify the <code>.env</code> file&#39;s <code>IO_THREADS</code> parameter to 5. </p>\n<pre><code class=\"language-bash\">#.env\nVALKEY_PASSWORD=e41fb9818502071d592b36b99f63003019861dad\nNODE_IP=&lt;VALKEY SERVER IP&gt;\nIO_THREADS=5\n</code></pre>\n<p>Then we can <code>docker compose -f valkey.yaml up -d</code> to start it again. Remote into the benchmarking server to start the test and...?</p>\n<pre><code class=\"language-bash\">redis-benchmark -n 10000000 -t set,get -P 16 -q -a e41fb9818502071d592b36b99f63003019861dad --threads 5 -h 10.0.1.136\nSET: 345494.75 requests per second, p50=0.911 msec                    \nGET: 327858.09 requests per second, p50=0.879 msec  \n</code></pre>\n<p>Wait a second... These results are worse than the ones before? We went from 416k requests per second to 336k.... What&#39;s happening?</p>\n<p><img src=\"/content/blog/2024-11-21-testing-the-limits/images/io_threads.png\" alt=\"A picture of the valkey server with 4 cores represented as boxes. In each of the four core boxes there are other boxes. 2 cores have 1 IO Thread Box, 1 Core has 2 IO thread boxes, and the last one has 1 IO Thread box along with a Valkey Process box.\" /></p>\n<p>We have over-subscribed our CPU. This means we&#39;ve created more worker threads than CPU cores. When a thread is under constant load it is competing with other threads on that core for resources. Not to mention, they are also competing with the Valkey process for resources. </p>\n<p>That&#39;s why <a href=\"https://github.com/valkey-io/valkey/blob/a62d1f177b7888ec88035a0a1ce600fbc2280ce7/valkey.conf#L1337-L1341\">Valkey recommends</a> setting the number of threads to be a value less than the number of cores you have. For our little 4 core server lets change the <code>IO_THREADS</code> parameter to be 2 threads in the <code>.env</code> file and try again.</p>\n<pre><code class=\"language-bash\">redis-benchmark -n 10000000 -t set,get -P 16 -q -a e41fb9818502071d592b36b99f63003019861dad --threads 5 -h 10.0.1.136\nSET: 609050.44 requests per second, p50=0.831 msec                    \nGET: 521186.22 requests per second, p50=0.719 msec\n</code></pre>\n<p>Much better! Now we are seeing around 565,000 requests per second. Thats a 35% increase in performance across both metrics! Not to mention in the picture below you can see that we have 100% utilization across all of our CPU&#39;s which means there&#39;s no more room for improvement! </p>\n<p><img src=\"/content/blog/2024-11-21-testing-the-limits/images/io_threading_htop.png\" alt=\"A picture of an HTOP window showing all four of our CPU&#39;s at 100% utilization.\" /></p>\n<p>Right? Well believe it or not we can squeeze even more performance out of our little CM4! </p>\n<p><img src=\"/content/blog/2024-11-21-testing-the-limits/images/io_threading_arch.png\" alt=\"A picture of our Valkey server with the 4 core boxes and to the right of them is a memory box. In the first core box is the Valkey process and in the next two there are IO Threads. The valkey process has a loop showing it communiacting with both of the IO threads. It also has a bracket showing it managing all the memory.\" /></p>\n<p>Above is a representitive outline of what&#39;s happening on the server. The Valkey process has to take up valuble cycles managing the IO Threads. Not only that it has to perform a lot of work to manage all the memory assigned to it. That&#39;s a lot of work for a single process.</p>\n<p>Now there is actually one more optimization we can use to make single threaded Valkey even faster. Valkey recently has done a substantial amount of work to support speculative execution. This work allows Valkey to predict which values will be needed from memory in future processing steps. This way Valkey server doesn&#39;t have to wait for memory access which is an order of magnitude slower than L1 caches. While I won&#39;t go through the details of how this works as there&#39;s already a <a href=\"https://valkey.io/blog/unlock-one-million-rps-part2/\">great blog that describes how to take advantage of these optimizations</a>. Here are the results: </p>\n<pre><code class=\"language-bash\">redis-benchmark -n 10000000 -t set,get -P 16 -q -a e41fb9818502071d592b36b99f63003019861dad --threads 5 -h 10.0.1.136\nSET: 632791.25 requests per second, p50=1.191 msec\nGET: 888573.00 requests per second, p50=0.695 msec\n</code></pre>\n<p>While these results are better they are a bit confusing. After talking with some of Valkey&#39;s maintainers it seems there may be something different in the way that Rasbian is configured when it comes to memory writes. In their testing the <code>GET/SET</code> requests were nearly identical but in my testing so far the write speed seems to always be behind read speed. If you think you know why please reach out!</p>\n<h2>Clustered Valkey</h2>\n<p><img src=\"/content/blog/2024-11-21-testing-the-limits/images/valkey_clustered.png\" alt=\"A picture of our Valkey server with the 4 core boxes and to the right of them is a memory box. In the first three core boxs are Valkey processes. Each of them has a bracket around a portion of the memory.\" /></p>\n<p>For our last step we are going to spin up a Valkey cluster. This cluster will have individual instances of Valkey running that each will be responsible for managing their own keys. This way each instance can execute operations in parallel much more easily.  </p>\n<p>I am not going into detail with how the keyspaces work but <a href=\"https://valkey.io/topics/cluster-tutorial/\">here is a good 101 guide</a> for understanding clustering in Valkey. </p>\n<p>First we&#39;ll stop our previous Valkey container <code>docker compose -f valkey.yaml down</code>. Now we can create our docker compose file for the cluster. Because each of these are exposed on the host they will all need to be using different ports. Additionally, all of them need to be aware they are started in cluster mode so they can redirect requests to the appropriate instance. </p>\n<pre><code class=\"language-yaml\"># valkey-cluster.yaml\nservices:\n  valkey-node-1:\n    hostname: valkey1\n    image: valkey/valkey:latest\n    command: valkey-server --port 6379 --cluster-enabled yes --cluster-config-file nodes.conf --cluster-node-timeout 5000 --requirepass ${VALKEY_PASSWORD} --save &quot;&quot;\n    volumes:\n      - ./data1:/data\n    network_mode: host\n  valkey-node-2:\n    hostname: valkey2\n    image: valkey/valkey:latest\n    command: valkey-server --port 6380 --cluster-enabled yes --cluster-config-file nodes.conf --cluster-node-timeout 5000 --requirepass ${VALKEY_PASSWORD} --save &quot;&quot;\n    volumes:\n      - ./data2:/data\n    network_mode: host\n  valkey-node-3:\n    hostname: valkey3\n    image: valkey/valkey:latest\n    command: valkey-server --port 6381 --cluster-enabled yes --cluster-config-file nodes.conf --cluster-node-timeout 5000 --requirepass ${VALKEY_PASSWORD} --save &quot;&quot;\n    volumes:\n      - ./data3:/data\n    network_mode: host\n\nvolumes:\n  data1:\n    driver: local\n  data3:\n    driver: local\n  data2:\n    driver: local\n</code></pre>\n<p>Run <code>docker compose -f valkey-cluster.yaml up -d</code> to start the cluster. There is one more step to get the cluster running. Find the name of one of your nodes with <code>docker ps --format &#39;{{.Names}}&#39;</code>. </p>\n<pre><code class=\"language-bash\">docker ps --format &#39;{{.Names}}\nkvtest-valkey-node-1-1\nkvtest-valkey-node-3-1\nkvtest-valkey-node-2-1\n</code></pre>\n<p>I&#39;ll use the first container to finish the cluster creation. Once the containers have started up we have to tell them the details they need to use for the cluster. Below I am using the IP of the host and the port configurations of all the containers to create the cluster. This is because these addresses need to be accessible from the benchmarking server. </p>\n<pre><code class=\"language-bash\">docker exec -it kvtest-valkey-node-1-1 valkey-cli --cluster create 10.0.1.136:6379 10.0.1.136:6380 10.0.1.136:6381 -a e41fb9818502071d592b36b99f63003019861dad\n</code></pre>\n<p>Now we can run our benchmark! We need to add the <code>--cluster</code> flag to our benchmarking command. Also, because this is so fast I ended up moving from 1 million requests to 10 million requests. That way we can make sure Valkey has time to fully utilize all it&#39;s resources. </p>\n<pre><code class=\"language-bash\">redis-benchmark -n 10000000 -t set,get -P 16 -q --threads 10 --cluster -a e41fb9818502071d592b36b99f63003019861dad --threads 5 -h 10.0.1.136 \nCluster has 3 master nodes:\n\nMaster 0: 219294612b44226fa32482871cf21025ff531875 10.0.1.136:6380\nMaster 1: e5d85b970551c27065f1552f5358f4add6114d98 10.0.1.136:6381\nMaster 2: 1faf3d0dd22e518eec11fd46c0de6ce18cd15cfe 10.0.1.136:6379\n\nSET: 1122838.50 requests per second, p50=0.575 msec                     \nGET: 1188071.75 requests per second, p50=0.511 msec  \n</code></pre>\n<p><strong>1,155,000 requests per second</strong>. We&#39;ve managed to double our requests per second. All this on a single board computer that&#39;s the size of a credit card. </p>\n<p>While this is far from what I would recommend for a production server these are the same steps I&#39;d recommend to someone evaluating Valkey. It&#39;s important test with a single instance to start finding the optimal settings. Then you can begin to scale up your test by adding either more IO Threads or Valkey instances. </p>\n<p>Testing should mirror your production workload as best it can. This test is using synthetic data. That&#39;s why I&#39;d recommend checking out the documentation to find what other settings you may need to test with. For example, we tested with the default settings of 50 client connections and 3 byte payloads. Your production workload may look different so explore all the settings! You may find that IO threading works better for your use case than I did in this case.</p>\n<p>If you enjoyed this read make sure to check out my blog <a href=\"https://tippybits.com\">TippyBits.com</a> where I post content like this on a regular basis. Stay curious my friends!</p>\n",
    "slug": "testing-the-limits",
    "category": "news",
    "imageUrl": "/assets/media/blog/random-05.webp",
    "authorUsernames": [
      "dtaivpp"
    ],
    "trending": false
  },
  {
    "title": "Generally Available: Valkey 8.0.0",
    "date": "2024-09-16T07:01:01.000Z",
    "excerpt": "Today marks a milestone for the Valkey project: the first major release.",
    "content": "<p>The first ever release of Valkey, 7.2.5, became generally available more than 5 months ago.\nWhile the initial release was a milestone, it focused on compatibility and license continuity; bringing no new features to the table.\nToday marks a different milestone for the Valkey project: the first major release.\nValkey 8.0.0 continues the traditions of the seven major versions of Redis that precede it by bringing improvements to speed and efficiency alongside new features.</p>\n<p>Key properties of the Valkey project are transparency and collaboration.\nAs a consequence of Valkey 8.0.0 being developed entirely in the open, the team has already written about both the big and small features of the release.\nThe best overview is the RC1 blog which breaks down all the changes and features in the release into a few sections: <a href=\"/blog/valkey-8-0-0-rc1/#performance\">performance</a>, <a href=\"/blog/valkey-8-0-0-rc1/#reliability\">reliability</a>, <a href=\"/blog/valkey-8-0-0-rc1/#replication\">replication</a>, <a href=\"https://valkey.io/blog/valkey-8-0-0-rc1/#observability\">observability</a>, and <a href=\"/blog/valkey-8-0-0-rc1/#efficiency\">efficiency</a>.\nAdditionally, there are deep dives on the <a href=\"/blog/unlock-one-million-rps/\">speed</a> (with a <a href=\"/blog/unlock-one-million-rps-part2/\">follow up</a>) and <a href=\"/blog/valkey-memory-efficiency-8-0/\">efficiency</a> improvements in Valkey 8.0.0.</p>\n<p>While this is a major version, Valkey takes command set compatibility seriously: Valkey 8.0.0 makes no backwards incompatible changes to the existing command syntax or their responses.\nYour existing tools and custom software will be able to immediately take advantage of Valkey 8.0.0.\nSince Valkey 8.0.0 does make some small changes to previously undefined behaviors, it&#39;s wise to <a href=\"https://github.com/valkey-io/valkey/blob/8.0.0/00-RELEASENOTES\">read the release notes</a>.\nAdditionally, because this version makes changes in how the software uses threading, you may want to re-evaluate your cluster’s infrastructure to achieve the highest performance.</p>\n<p>Valkey 8.0.0 has gone through multiple rounds of release candidates, testing, and verification.\nThe Technical Steering Committee considers it ready for production usage.\nYou can <a href=\"https://github.com/valkey-io/valkey/tree/8.0.0\">build from source</a>, start <a href=\"/download/\">installing the binaries, or deploy the containers</a> today.\nExpect package managers to pick up the latest version in the coming days.</p>\n<p><strong>Note:</strong> <a href=\"https://github.com/valkey-io/valkey/tree/8.0.1\">Valkey 8.0.1</a> was released on October 2, read the release notes on <a href=\"https://github.com/valkey-io/valkey/blob/8.0.1/00-RELEASENOTES\">GitHub</a>.</p>\n",
    "slug": "valkey-8-ga",
    "category": "news",
    "imageUrl": "/assets/media/blog/random-03.webp",
    "authorUsernames": [
      "kyledvs"
    ],
    "trending": false
  },
  {
    "title": "Unlock 1 Million RPS: Experience Triple the Speed with Valkey - part 2",
    "date": "2024-09-13T07:01:01.000Z",
    "excerpt": "Maximize the performance of your hardware with memory access amortization",
    "content": "<p> In the <a href=\"/blog/unlock-one-million-rps/\">first part</a> of this blog, we described how we offloaded almost all I/O operations to I/O threads, thereby freeing more CPU cycles in the main thread to execute commands. When we profiled the execution of the main thread, we found that a considerable amount of time was spent waiting for external memory. This was not entirely surprising, as when accessing random keys, the probability of finding the key in one of the processor caches is relatively low.  Considering that external memory access latency is approximately 50 times higher than L1 cache, it became clear that despite showing 100% CPU utilization, the main process was mostly “waiting”. In this blog, we describe the technique we have been using to increase the number of parallel memory accesses, thereby reducing the impact that external memory latency has on performance.</p>\n<h3>Speculative execution and linked lists</h3>\n<p>Speculative execution is a performance optimization technique used by modern processors, where the processor guesses the outcome of conditional operations and executes in parallel subsequent instructions ahead of time. Dynamic data structures, such as linked lists and search trees, have many advantages over static data structures: they are economical in memory consumption, provide fast insertion and deletion mechanisms, and can be resized efficiently. However, some dynamic data structures have a major drawback: they hinder the processor&#39;s ability to speculate on future memory load instructions that could be executed in parallel. This lack of concurrency is especially problematic in very large dynamic data structures, where most pointer accesses result in high-latency external memory access.</p>\n<p>In this blog, Memory Access Amortization, a method that facilitates speculative execution to improve performance, is introduced along with how it is applied in Valkey. The basic idea behind the method is that by interleaving the execution of operations that access random memory locations, one can achieve significantly better performance than by executing them serially.</p>\n<p>To depict the problem we are trying to solve consider the following <a href=\"/assets/C/list_array.c\">function</a> which gets an array of linked list and returns sum of all values in the lists:</p>\n<pre><code class=\"language-c\">unsigned long sequentialSum(size_t arr_size, list **la) {\n    list *lp;\n    unsigned long  res = 0; \n\n    for (int i = 0; i &lt; arr_size; i++) { \n        lp = la[i]; \n        while (lp) { \n            res += lp-&gt;val;\n            lp = lp-&gt;next;\n        }\n    }\n\n    return res; \n}\n</code></pre>\n<p>Executing this function on an array of 16 lists containing 10 million elements each takes approximately 20.8 seconds on an ARM processor (Graviton 3). Now consider the following alternative implementation which instead of scanning the lists separately,  interleaves the executions of the lists scans: </p>\n<pre><code class=\"language-c\">unsigned long interleavedSum(size_t arr_size, list **la) {\n    list **lthreads = malloc(arr_size * sizeof(list *)); \n    unsigned long res = 0; \n    int n = arr_size; \n\n    for (int i = 0; i &lt; arr_size; i++) {\n        lthreads[i] = la[i]; \n        if (lthreads[i] == NULL) \n            n--; \n    } \n\n    while(n) {\n        for (int i = 0; i &lt; arr_size; i++) { \n            if (lthreads[i] == NULL) \n                continue; \n            res += lthreads[i]-&gt;val;\n            lthreads[i] = lthreads[i]-&gt;next; \n            if (lthreads[i] == NULL) \n                n--;\n        }  \n    }\n\n    free(lthreads);\n    return res; \n}\n</code></pre>\n<p>Running this new version with the same input as previously described takes less than 2 seconds, achieving a 10x speedup! The explanation for this significant improvement lies in the processor&#39;s speculative execution capabilities. In a standard sequential traversal of a linked list, as seen in the first version of the function, the processor cannot &#39;speculate&#39; on future memory access instructions. This limitation becomes particularly costly with large lists, where each pointer access likely results in a expensive external memory access. In contrast, the alternative implementation, which interleaves list traversals, allows the processor to issue more memory accesses in parallel. This leads to an overall reduction in memory access latency through amortization. </p>\n<p>One way to maximize the amount of parallel memory access issued is to add prefetch instructions. Replacing </p>\n<pre><code class=\"language-c\">             if (lthreads[i] == NULL) \n                n--;\n</code></pre>\n<p>with</p>\n<pre><code class=\"language-c\">            if (lthreads[i]) \n                __builtin_prefetch(lthreads[i]);\n            else \n                n--;\n</code></pre>\n<p>reduces the execution time further to 1.8 sec. </p>\n<h3>Back to Valkey</h3>\n<p>In the first part, we described how we updated the existing I/O threads implementation to increase parallelism and reduce the amount of I/O operations executed by the main thread to a minimum. Indeed, we observed an increase in the number of requests per second, reaching up to 780K SET commands per second. Profiling the execution revealed that Valkey&#39;s main thread was spending more than 40% of its time in a single function: lookupKey, whose goal is to locate the command keys in Valkey&#39;s main dictionary. This dictionary is implemented as a straightforward chained hash, as shown in the picture below: \n<img src=\"/content/blog//assets/media/pictures/lookupKey.jpg\" alt=\"dict find\" />\nOn a large enough set of keys, almost every memory address accessed while searching the dictionary will not be found in any of the processor caches, resulting in costly external memory accesses. Also, similarly as with the linked list from above, since the addresses in the table→dictEntry→...dictEntry→robj sequence are serially dependent, it is not possible to determine the next address to be accessed before the previous address in the chain has been resolved.  </p>\n<h3>Batching and interleaving</h3>\n<p>To overcome this inefficiency, we adopted the following approach. Every time a batch of incoming commands from the I/O threads is ready for execution, Valkey’s main thread efficiently prefetches the memory addresses needed for future lookupKey invocations for the keys involved in the commands  before executing the commands. This prefetch phase is achieved by dictPrefetch, which, similarly as with the linked list example from above, interleaves the table→dictEntry→...dictEntry→robj search sequences for all keys. This reduces the time spent on lookupKey by more than 80%. Another issue we had to address was that all the incoming parsed commands from the I/O threads were not present in the L1/L2 caches of the core running Valkey’s main thread. This was also resolved using the same method.  All the relevant code can be found in <a href=\"https://github.com/valkey-io/valkey/blob/unstable/src/memory_prefetch.c\">memory_prefetch.c</a>. In total the impact of the memory access amortization on Valkey performance is almost 50% and it increased the requests per second to more than 1.19M rps. </p>\n<h3>How to reproduce Valkey 8.0 performance numbers</h3>\n<p>This section will walk you through the process of reproducing our performance results, where we achieved 1.19 million requests per second using Valkey 8.</p>\n<h3>Hardware Setup</h3>\n<p>We conducted our tests on an AWS EC2 c7g.4xlarge instance, featuring 16 cores on an ARM-based (aarch64) architecture.</p>\n<h3>System Configuration</h3>\n<blockquote>\n<p>Note: The core assignments used in this guide are examples. Optimal core selection may vary depending on your specific system configuration and workload.</p>\n</blockquote>\n<p>Interrupt affinity - locate the network interface with <code>ifconfig</code> (let&#39;s assume it is <code>eth0</code>) and its associated IRQs with </p>\n<pre><code class=\"language-bash\">grep eth0 /proc/interrupts | awk &#39;{print $1}&#39; | cut -d : -f 1\n</code></pre>\n<p>In our setup, lines <code>48</code> to <code>55</code> are allocated for <code>eth0</code> interrupts. Allocate one core per 4 IRQ lines: </p>\n<pre><code class=\"language-bash\">for i in {48..51}; do echo 1000 &gt; /proc/irq/$i/smp_affinity; done\nfor i in {52..55}; do echo 2000 &gt; /proc/irq/$i/smp_affinity; done\n</code></pre>\n<p>Server configuration - launch the Valkey server with these minimal configurations:</p>\n<pre><code class=\"language-bash\">./valkey-server --io-threads 9 --save --protected-mode no\n</code></pre>\n<p><code>--save</code> disables dumping to RDB file and <code>--protected-mode no </code>  allows connections from external hosts. <code>--io-threads</code> number includes the main thread and the IO threads, meaning that in our case 8 I/O threads are launched in addition to the main thread. </p>\n<p>Main thread affinity - pin the main thread to a specific CPU core, avoiding the cores handling IRQs. Here we use core #3:</p>\n<pre><code class=\"language-bash\">sudo taskset -cp 3 `pidof valkey-server`\n</code></pre>\n<blockquote>\n<p>Important: We suggest experimenting with different core pinning strategies to find the optimal performance while avoiding conflicts with IRQ-handling cores.</p>\n</blockquote>\n<h3>Benchmark Configuration</h3>\n<p>Run the benchmark from a separate instance using the following parameters:</p>\n<ul>\n<li>Value size: 512 bytes</li>\n<li>Number of keys: 3 million</li>\n<li>Number of clients: 650</li>\n<li>Number of threads: 50 (may vary for optimal results)</li>\n</ul>\n<pre><code class=\"language-bash\">./valkey-benchmark -t set -d 512 -r 3000000 -c 650 --threads 50 -h &quot;host-name&quot; -n 100000000000\n</code></pre>\n<blockquote>\n<p>Important: When running the benchmark, it may take a few seconds for the database to get populated and for the performance to stabilize. You can adjust the <code>-n</code> parameter to ensure the benchmark runs long enough to reach optimal throughput.</p>\n</blockquote>\n<h3>Testing and Availability</h3>\n<p><a href=\"https://github.com/valkey-io/valkey/releases/tag/8.0.0-rc2\">Valkey 8.0 RC2</a> is available now for evaluation with I/O threads and memory access amortization.</p>\n",
    "slug": "unlock-one-million-rps-part2",
    "category": "news",
    "imageUrl": "/assets/media/blog/random-05.webp",
    "authorUsernames": [
      "dantouitou",
      "uriyagelnik"
    ],
    "trending": false
  },
  {
    "title": "Storing more with less: Memory Efficiency in Valkey 8",
    "date": "2024-09-04T07:01:01.000Z",
    "excerpt": "Learn about the new memory efficiency improvements in Valkey 8 which reduces memory overhead, which allows more data to be stored in the same amount of memory.",
    "content": "<p>Valkey 8.0 GA is around the corner and one of the themes is increasing overall memory efficiency. Memory overhead reduction has the obvious effect of better resource utilization, but also impacts performance. By minimizing unnecessary memory consumption, you can store more data with the same hardware resources and improve overall system responsiveness. This post is going to give an overview into how Valkey internally manages the data and its memory overhead. Additionally, it talks about the two major improvements for Valkey 8.0 that improves the overall memory efficiency.</p>\n<h2>Overview</h2>\n<p>Valkey has two modes of operation: standalone and cluster mode. Standalone allows for one primary with it’s replica(s). To shard data horizontally and scale to store large amounts of data, cluster mode provides a mechanism to set up multiple primaries each with their own replica(s). </p>\n<p><img src=\"/content/blog//assets/media/pictures/valkey_operation_mode.png\" alt=\"Figure 1 Standalone (left) and Cluster mode (right)\" /></p>\n<p>For both standalone and cluster mode setup, Valkey&#39;s main dictionary is a hash table with a chained linked list: The major components are a <strong>bucket</strong> and <strong>dictionary entry</strong>. A key is hashed to a bucket and each bucket points to a linked list of dictionary entries and further each dictionary entry consists of key, value, and a next pointer. Each pointer takes 8 bytes of memory usage. So, a single dictionary entry has a minimum overhead of 24 bytes.</p>\n<p><img src=\"/content/blog//assets/media/pictures/dictionary_bucket_and_entry_overview.png\" alt=\"Figure 2 Dictionary bucket pointing to a dictionary entry\" /></p>\n<p>In cluster mode, Valkey uses a concept called <a href=\"https://valkey.io/topics/cluster-tutorial/\">hash slots</a> to shard data. There are 16,384 hash slots in cluster, and to compute the hash slot for a given key, the server computes the CRC16 of the key modulo 16,384. Keys are distributed on basis of these slots assigned to each of the primary. The server needs to maintain additional metadata for bookkeeping i.e. slot-to-key mapping to move a slot from one primary to another. In order to maintain the slot to key mapping, two additional pointers <code>slot-prev</code> and <code>slot-next</code> (Figure 3)  are stored as metadata in each dictionary entry forming a double linked list of all keys belonging to a given slot. This further increases the overhead by 16 bytes per dictionary entry i.e. total 40 bytes.</p>\n<p><img src=\"/content/blog//assets/media/pictures/dictionary_in_cluster_mode_7.2.png\" alt=\"Figure 3 Dictionary in cluster mode (Valkey 7.2) with multiple key value pair\" /></p>\n<h2>Improvements</h2>\n<h3>Optimization 1 - <a href=\"https://github.com/redis/redis/pull/11695\">Dictionary per slot</a></h3>\n<p>The first optimization is a dictionary per slot (16,384 of them in total), where each dictionary stores data for a given slot. With this simplification, the cost of maintaining additional metadata for the mapping of slot to key is no longer required in Valkey 8. To iterate over all the keys in a given slot, the engine simply finds out the dictionary for a given slot and traverse all the entries in it. This reduces the memory usage per dictionary entry by 16 bytes with a small memory overhead around 1 MB per node. As cluster mode is generally used for storing large amount of keys, avoiding the additional overhead per key allows users to store more number of keys in the same amount of memory.</p>\n<p><img src=\"/content/blog//assets/media/pictures/dictionary_in_cluster_mode_8.0.png\" alt=\"Figure 4 Dictionary in cluster mode (Valkey 8.0) with multiple key value pair\" /></p>\n<p>A few of the interesting challenges that comes up with the above improvements are supporting existing use cases which were optimized with a single dictionary for the entire keyspace. The usecases are:</p>\n<ul>\n<li><a href=\"https://valkey.io/commands/scan/\">Iterating the entire keyspace</a> - Command like <code>SCAN</code> to iterate over the entire keyspace.</li>\n<li><a href=\"https://valkey.io/topics/lru-cache/\">Random key for eviction</a> - The server does random sampling of the keyspace to find ideal candidate for eviction.</li>\n<li><a href=\"https://valkey.io/commands/randomkey/\">Finding a random key</a> - Commands like <code>RANDOMKEY</code> retrieve a random key from the database.</li>\n</ul>\n<p>In order to efficiently implement these functions, we need to be able to both find non-empty slots, to skip over empty slots during scanning, and be able to select a random slot weighted by the number of keys that it owns. These requirements require a data structure which provides the following functionality:</p>\n<ol>\n<li>Modify value for a given slot - If a key gets added or removed, increment or decrement the value for that given slot by 1 respectively.</li>\n<li>Cumulative frequency until each slot - For a given number representing a key between 1 and the total number of keys, return the slot which covers the particular key.</li>\n</ol>\n<p>If approached naively, the former and latter operation would take O(1) and O(N) respectively. However, we want to minimize the latter operation’s time complexity and minimally avoid in the former. Hence, a <a href=\"https://www.topcoder.com/thrive/articles/Binary%20Indexed%20Trees\">binary indexed tree (BIT) or fenwick tree</a> which provides the above functionality with a minimal memory overhead (~1 MB per node) and the time complexity is also bounded to O(M log N) for both the operations where M = number of modification(s) and N = number of slots. This enables skipping over empty slots efficiently while iterating over the keyspace as well as finding a slot for a given key index in logarithmic time via binary search over the cumulative sum maintained by the BIT. </p>\n<p>Another interesting side effect is on the rehashing operation. Rehashing is CPU intensive. By default, a limited number of buckets are allocated in a dictionary and it expands/shrinks dynamically based on usage. While undergoing rehashing, all the data needs to be moved from an old dictionary to a new dictionary. With Valkey 7.2, a global dictionary being shared across all the slots, all the keys get stored under a single dictionary and each time the fill factor (number of keys / number of buckets) goes above 1, the dictionary needs to move to a larger dictionary (multiple of 2) and move a large amount of keys. As this operation is performed on the fly, it causes an increase in latency for regular command operations while it&#39;s ongoing. With the per-slot dictionary optimization, the impact of rehashing is localized to the specific dictionary undergoing the process and only a subset of keys needs to be moved.</p>\n<p>Overall, with this new approach, the benefits are: </p>\n<ol>\n<li>Removes additional memory overhead in cluster mode: Get rid of two pointers (16 bytes) per key to keep the mapping of slot to keys.</li>\n<li>With the rehashing operation spread out across dictionaries, CPU utilization is also spread out.</li>\n</ol>\n<h3>Optimization 2 - <a href=\"https://github.com/valkey-io/valkey/pull/541\">Key embedding into dictionary entry</a></h3>\n<p>After the dictionary per slot change, the memory layout of dictionary entry in cluster mode is the following, there are three pointers (key, value, and next). The key pointer points to a SDS (<a href=\"https://github.com/antirez/sds/blob/master/README\">simple dynamic string</a>) which contains the actual key data. As a key is immutable, without bringing in much complexity, it can be embedded into the dictionary entry which has the same lifetime as the former.</p>\n<p><img src=\"/content/blog//assets/media/pictures/key_embedding.png\" alt=\"Figure 5 Key data storage in 7.2 (left) and 8.0 (right)\" /></p>\n<p>With this new approach, the overall benefits are: </p>\n<ol>\n<li>Reduces 8 bytes additional memory overhead per key.</li>\n<li>Removes an additional memory lookup for key: With access of dictionary entry, the additional random pointer access for key is no longer required leading to better cache locality and overall better performance.</li>\n</ol>\n<h3>Benchmarking</h3>\n<h4>Setup</h4>\n<p>A single shard cluster is setup with 1 primary and 2 replica(s). Each node runs with different version to highlight the memory improvements with each optimization introduced between 7.2 to 8.0 and to signify that no additional configuration is required to achieve the memory efficiency.</p>\n<ul>\n<li>Node A: Primary running on port 6379 with <a href=\"https://github.com/valkey-io/valkey/commit/ad0a24c7421d3a8ea76cf44b56001e3b3b6ed545\">Valkey 7.2 version</a></li>\n<li>Node B: Replica 1 running on port 6380 with <a href=\"https://github.com/valkey-io/valkey/commit/1ea49e5845a11250a13273c725720822c26860f1\">optimization 1 - dictionary per slot</a></li>\n<li>Node C: Replica 2 running on port 6381 with <a href=\"https://github.com/valkey-io/valkey/commit/a323dce8900341328114b86a92078c50cec0d9b8\">optimization 1 - dictionary per slot and optimization 2 - key embedding</a> - Includes all memory efficiency optimization in Valkey 8.</li>\n</ul>\n<h4>Synthetic data generation using <a href=\"https://valkey.io/topics/benchmark/\">valkey-benchmark utility</a></h4>\n<pre><code>src/valkey-benchmark \\\n -t set \\\n -n 10000000 \\\n -r 10000000 \\\n -d 16\n</code></pre>\n<h4>Memory Usage</h4>\n<ul>\n<li>Node A</li>\n</ul>\n<pre><code>127.0.0.1:6379&gt; DBSIZE # command to retrieve number of keys.\n(integer) 6318941\n127.0.0.1:6379&gt; INFO MEMORY # command to retrieve statistics about memory usage\n# Memory\nused_memory:727339288\nused_memory_human:693.64M\n</code></pre>\n<ul>\n<li>Node B</li>\n</ul>\n<pre><code>127.0.0.1:6380&gt; DBSIZE # command to retrieve number of keys.\n(integer) 6318941\n127.0.0.1:6380&gt; INFO MEMORY # command to retrieve statistics about memory usage\n# Memory\nused_memory:627851888\nused_memory_human:598.77M\n</code></pre>\n<ul>\n<li>Node C</li>\n</ul>\n<pre><code>127.0.0.1:6381&gt; DBSIZE # command to retrieve number of keys.\n(integer) 6318941\n127.0.0.1:6381&gt; INFO MEMORY # command to retrieve statistics about memory usage\n# Memory\nused_memory:577300952\nused_memory_human:550.56M\n</code></pre>\n<h4>Overall Improvement</h4>\n<p><img src=\"/content/blog//assets/media/pictures/memory_usage_comparison.png\" alt=\"Figure 6 Overall memory usage with benchmark data\" /></p>\n<h4>With dictionary per slot change memory usage reduced from 693.64 MB to 598.77 MB with the same dataset</h4>\n<ul>\n<li><strong>Percentage Drop 1</strong>: ((693.64 - 598.77) / 693.64) * 100 = (94.87 / 693.64) * 100 ≈ 13.68%</li>\n</ul>\n<h4>Further with key embedding, memory usage reduced from 598.77 MB to 550.56 MB with the same dataset</h4>\n<ul>\n<li><strong>Percentage Drop 2</strong>: ((598.77 - 550.56) / 598.77) * 100 = (48.21 / 598.77) * 100 ≈ 8.05%</li>\n</ul>\n<h4>Overall Drop: From 693.64 MB to 550.56 MB</h4>\n<ul>\n<li><strong>Overall Percentage Drop</strong>: ((693.64 - 550.56) / 693.64) * 100 = (143.08 / 693.64) * 100 ≈ 20.63%</li>\n</ul>\n<p>So, the drop in percentage is approximately <strong>20.63% in overall memory usage on a given node on upgrade from Valkey 7.2 to Valkey 8.0</strong>.</p>\n<h2>Conclusion</h2>\n<p>Through the memory efficiency achieved by introducing dictionary per slot and key embedding into dictionary entry, users should have additional capacity to store more keys per node in Valkey 8.0 (up to 20%, but it will vary based on the workload). For users, upgrading from Valkey 7.2 to Valkey 8.0, the improvement should be observed automatically and no configuration changes are required. \nGive it a try by spinning up a <a href=\"https://valkey.io/download/\">Valkey cluster</a> and join us in the <a href=\"https://github.com/valkey-io/valkey/\">community</a> to provide feedback. Further, there is an ongoing discussion around overhauling the main dictionary with a more compact memory layout and introduce an open addressing scheme which will significantly improve memory efficiency. More details can be found <a href=\"https://github.com/valkey-io/valkey/issues/169\">here</a>.</p>\n",
    "slug": "valkey-memory-efficiency-8-0",
    "category": "news",
    "imageUrl": "/assets/media/blog/random-02.webp",
    "authorUsernames": [
      "hpatro"
    ],
    "trending": true
  },
  {
    "title": "Unlock 1 Million RPS: Experience Triple the Speed with Valkey",
    "date": "2024-08-05T07:01:01.000Z",
    "excerpt": "Learn about the new performnace improvements in Valkey 8 which reduces cost, improves latency and makes our environment greener.",
    "content": "<p>Valkey 8.0, set for release in September 2024, will bring major performance enhancements through a variety of improvements including a new multi-threaded architecture.\nThis update aims to significantly boost throughput and reduce latency across various hardware configurations.\nRead on to learn more about the new innovative I/O threading implementation and its impact on performance and efficiency.\nThis post is the first in a two-part series. The next post will dive into the new prefetch mechanism and its impact on performance.</p>\n<h3>Our Commitment to performance and efficiency</h3>\n<p>At AWS, we have hundreds of thousands of customers using Amazon ElastiCache and Amazon MemoryDB.\nFeedback we continuously hear from end users is that they need better absolute performance and want to squeeze more performance from their clusters.</p>\n<p>Our commitment to meeting these performance and efficiency needs led us down a path of improving the multi-threaded performance of our ElastiCache and MemoryDB services, through features we called <a href=\"https://aws.amazon.com/blogs/database/boosting-application-performance-and-reducing-costs-with-amazon-elasticache-for-redis/\">Enhanced IO</a> and <a href=\"https://aws.amazon.com/blogs/database/enhanced-io-multiplexing-for-amazon-elasticache-for-redis/\">Multiplexing</a>.\nToday we are excited to dive into how we are sharing our learnings from this performance journey by contributing a major performance improvement to the Valkey project.</p>\n<h3>Benefits of High Capacity Shards</h3>\n<p>Valkey&#39;s common approach to performance and memory improvement is scaling out by adding more shards to the cluster.\nHowever, the availability of more powerful nodes offers additional flexibility in application design.\nHigher-capacity shards can increase cluster capacity, improve resilience to request surges, and reduce latencies at high percentiles.\nThis approach is particularly beneficial for Valkey users with workloads that don&#39;t respond well to horizontal scaling, such as hot keys and large collections that can&#39;t be effectively distributed across multiple shards.</p>\n<p>Another challenge with horizontal scaling comes from multi-key operations like MGET.\nThese multi-key operations require all involved keys to reside in the same slot, often resulting in users utilizing only a small number of slots, which can significantly restrict the cluster&#39;s scalability potential.\nLarger shards can alleviate these constraints by accommodating more keys and larger collections within a single node.</p>\n<p>While larger shards offer these benefits, they come with trade-offs.\nFull synchronization for very large instances can be risky, and losing a large shard can be more impactful than losing a smaller one.\nConversely, managing a cluster with too many small instances can be operationally complex.\nThe optimal configuration depends on the specific workload, requiring a careful balance between scaling out and using larger shards.</p>\n<h3>Major Upgrade to Valkey Performance</h3>\n<p>Starting with version 8, Valkey users will benefit from an increase in multi-threaded performance, thanks to a new multi-threading architecture that can boost throughput and reduce latency on a wide range of hardware types.\n<img src=\"/content/blog//assets/media/pictures/performance_comparison.png\" alt=\"Performance comparison between existing I/O threading implementation and the new I/O threading implementation available in Valkey 8.\" /></p>\n<p>The data demonstrates a substantial performance improvement with the new I/O threads approach.\nThroughput increased by approximately 230%, rising from 360K to 1.19M requests per second compared to Valkey 7.2\nLatency metrics improved across all percentiles, with average latency decreasing by 69.8% from 1.792 ms to 0.542 ms.</p>\n<p>Tested with 8 I/O threads, 3M keys DB size, 512 bytes value size, and 650 clients running sequential SET commands using AWS EC2 C7g.16xlarge instance.\nPlease note that these numbers include the Prefetch change that will be described in the next <a href=\"/blog/unlock-one-million-rps-part2/\">blog post</a></p>\n<h3>Performance Without Compromising Simplicity</h3>\n<p>Valkey strives to stay simple by executing as much code in a single thread as possible.\nThis ensures an API that can continuously evolve without the need to use complex synchronization and avoid race conditions.\nOur new multi-threading approach is designed based on this long-standing architectural principle that we believe is the right architecture for Valkey.\nIt utilizes a minimal number of synchronization mechanisms and keeps Valkey command execution single-threaded, simple, and primed for future enhancements.</p>\n<p><img src=\"/content/blog//assets/media/pictures/io_threads.png\" alt=\"I/O threads high level design\" /></p>\n<h3>High Level Design</h3>\n<p>The above diagram depicts the high-level design of I/O threading in Valkey 8.\nI/O threads are worker threads that receive jobs to execute from the main thread. \nA job can involve reading and parsing a command from a client, writing responses back to the client, polling for I/O events on TCP connections, or deallocating memory.\nWhile I/O threads are busy handling I/O, the main thread is able to spend more time executing commands. </p>\n<p>The main thread orchestrates all the jobs spawned to the I/O threads, ensuring that no race conditions occur. \nThe number of active I/O threads can be adjusted by the main thread based on the current load to ensure efficient utilization of the underlying hardware. \nDespite the dynamic nature of I/O threads, the main thread maintains thread affinity, ensuring that, when possible the same I/O thread will handle I/O for the same client to improve memory access locality. </p>\n<p>Socket polling system calls, such as <code>epoll_wait</code>, are expensive procedures. \nWhen executed solely by the main thread, <code>epoll_wait</code> consumes more than 20 percent of the time. \nTherefore, we decided to offload <code>epoll_wait</code> execution to the I/O threads in the following way: to avoid race conditions, at any given time, at most one thread, either an io_thread or the main thread, executes <code>epoll_wait</code>. \nI/O threads never sleep on <code>epoll</code>, and whenever there are pending I/O operations or commands to be executed, <code>epoll_wait</code> calls are scheduled to the I/O threads by the main thread. \nIn all other cases, the main thread executes the <code>epoll_wait</code> with the waiting time as in the original Valkey implementation</p>\n<p>In addition, before executing commands, the main thread performs a new procedure, prefetch-commands-keys, which aims to reduce the number of external memory accesses needed when executing the commands on the main dictionary. A detailed explanation of the technique used in that procedure will be described in our next blog</p>\n<h3>Testing and Availability</h3>\n<p>The enhanced performance will be available for testing in the first release candidate of Valkey, available today.</p>\n",
    "slug": "unlock-one-million-rps",
    "category": "news",
    "imageUrl": "/assets/media/blog/random-05.webp",
    "authorUsernames": [
      "dantouitou",
      "uriyagelnik"
    ],
    "trending": false
  },
  {
    "title": "Valkey 8.0: Delivering Enhanced Performance and Reliability",
    "date": "2024-08-02T07:01:01.000Z",
    "excerpt": "The first release candidate of Valkey 8.0 is now available! Come learn about the exciting improvements in performance, reliability, and observability that are available in this new version.",
    "content": "<p>The Valkey community is proud to unveil the first release candidate of Valkey 8.0,\na major update designed to enhance performance, reliability, and observability \nfor all Valkey installations. In this blog, we&#39;ll dive a bit deeper into each of these \nareas and talk about the exciting features we&#39;ve built for this release.</p>\n<h2>Performance</h2>\n<p>Valkey 8.0 features significant improvements to the existing I/O threading system,\nallowing the main thread and I/O threads to operate concurrently. This release also\nincludes a number of improvements to offload work to the I/O threads and introduces\nefficient batching of commands. Altogether, Valkey 8.0 is designed to handle up to\n1.2 million Queries Per Second (QPS) on AWS&#39;s r7g platform, compared to the previous\nlimit of 380K QPS. We&#39;ll dive deeper into these numbers in an upcoming blog.</p>\n<p>NOTE: Not all improvements are available in the release candidate, but they will\nbe available in the GA release of Valkey 8.0.</p>\n<ul>\n<li><strong>Asynchronous I/O Threading</strong>: Enables parallel processing of commands and\nI/O operations, maximizing throughput and minimizing bottlenecks.</li>\n<li><strong>Intelligent Core Utilization</strong>: Distributes I/O tasks across multiple\ncores based on realtime usage, reducing idle time and improving energy efficiency.</li>\n<li><strong>Command Batching</strong>: Optimizes memory access patterns by prefetching frequently\naccessed data to minimize CPU cache misses, reducing memory accesses required for\ndictionary operations.</li>\n</ul>\n<p>For more details on these improvements, you can refer to\n<a href=\"https://github.com/valkey-io/valkey/pull/758\">#758</a> and\n<a href=\"https://github.com/valkey-io/valkey/pull/763\">#763</a>.</p>\n<h2>Reliability</h2>\n<p>Cluster scaling operations via slot migrations have historically been delicate.\nValkey 8.0 improves reliability and minimizes disruptions with the following\nenhancements:</p>\n<ul>\n<li><strong>Automatic Failover for Empty Shards</strong>: New shards that start empty, owning\nno slots, now benefit from automatic failover. This ensures high availability\nfrom the start of the scaling process.</li>\n<li><strong>Replication of Slot Migration States</strong>: All <code>CLUSTER SETSLOT</code> commands are\nnow replicated synchronously to replicas before execution on the primary. This\nreduces the chance of unavailability if the primary fails, as the replicas have\nthe most up-to-date information about the state of the shard. New replicas also\nautomatically inherit the state from the primary without additional input from\nan operator.</li>\n<li><strong>Slot Migration State Recovery</strong>: In the event of a failover, Valkey 8.0 automatically\nupdates the slot migration states on source and target nodes. This ensures requests\nare continuously routed to the correct primary in the target shard, maintaining\ncluster integrity and availability.</li>\n</ul>\n<p>For more details on these improvements, you can refer to\n<a href=\"https://github.com/valkey-io/valkey/pull/445\">#445</a>.</p>\n<h2>Replication</h2>\n<p>Valkey 8.0 introduces a dual-channel replication scheme, allowing the RDB and\nthe replica backlog to be transferred simultaneously, accelerating synchronization.</p>\n<ul>\n<li><strong>Reduced Memory Load</strong>: By streaming replication data to the replica during\nthe full sync, the primary node experiences significantly less memory pressure.\nThe replica now manages the Client Output Buffer (COB) tracking, reducing the\nlikelihood of COB overruns and enabling larger COB sizes on the replica side.</li>\n<li><strong>Reduced Parent Process Load</strong>: A dedicated connection for RDB transfer frees\nthe primary&#39;s parent process from handling this data, allowing it to focus on\nclient queries and improving overall responsiveness.</li>\n</ul>\n<p>Performance tests show improvements in write latency during sync, and in scenarios\nwith heavy read commands, the sync time can be cut by up to 50%. This translates\nto a more responsive system, even during synchronization.</p>\n<p>For more details on these improvements, you can refer to <a href=\"https://github.com/valkey-io/valkey/pull/60\">#60</a>.</p>\n<h2>Observability</h2>\n<p>Valkey 8.0 introduces a comprehensive per-slot metrics infrastructure, providing\ndetailed visibility into the performance and resource usage of individual slots.\nThis granular data helps inform decisions about resource allocation, load\nbalancing, and performance optimization.</p>\n<ul>\n<li><strong>Key Count</strong>: Returns the number of keys in each slot, making it easier to\nidentify the slots with the largest number of keys.</li>\n<li><strong>CPU Usage</strong>: Tracks CPU time consumed by operations on each slot, identifying\nareas of high utilization and potential bottlenecks.</li>\n<li><strong>Network Input/Output Bytes</strong>: Monitors data transmission and reception by\neach slot, offering insights into network load and bandwidth utilization.</li>\n<li><strong>Minimal Overhead</strong>: Initial benchmarks show that enabling detailed metrics\nincurs a negligible overhead of approximately 0.7% in QPS.</li>\n</ul>\n<p>For more details on these improvements, you can refer to <a href=\"https://github.com/valkey-io/valkey/pull/712\">#712</a>,\n<a href=\"https://github.com/valkey-io/valkey/pull/720\">#720</a>, and <a href=\"https://github.com/valkey-io/valkey/pull/771\">#771</a>.</p>\n<h2>Efficiency</h2>\n<p>Valkey 8.0 introduces two new improvements that reduce the memory overhead of keys,\nallowing users to store more data without any application changes.\nThe first change is that keys are now embedded in the main dictionary, eliminating separate\nkey pointers and significantly reducing memory overhead. This results in a 9-10%\nreduction in overall memory usage for scenarios with 16-byte keys and 8 or 16-byte\nvalues, along with performance improvements.</p>\n<p>This release also introduces a new per-slot dictionary for Valkey cluster, which\nreplaces a linked list that used to allow operator to list out all the keys in\na slot for slot-migration. The new architecture splits the main dictionary by slot,\nreducing the memory overhead by 16 bytes per key-value pair without degrading performance. </p>\n<p>For more details on these improvements, you can refer to <a href=\"https://github.com/valkey-io/valkey/pull/541\">#541</a>\nand <a href=\"https://github.com/redis/redis/pull/11695\">Redis#11695</a>.</p>\n<h2>Additional Highlights</h2>\n<ul>\n<li><strong>Dual IPv4 and IPv6 Stack Support</strong>: Seamlessly operate in mixed IP environments\nfor enhanced compatibility and flexibility.\nSee <a href=\"https://github.com/valkey-io/valkey/pull/736\">#736</a> for details.</li>\n<li><strong>Improved Pub/Sub Efficiency</strong>: Lightweight cluster messages streamline\ncommunication and reduce overhead for faster, more efficient Pub/Sub operations.\nSee <a href=\"https://github.com/valkey-io/valkey/pull/654\">#654</a> for details.</li>\n<li><strong>Valkey Over RDMA (Experimental)</strong>: Unlock significant performance improvements\nwith direct memory access between clients and Valkey servers, delivering up to\n275% increase in throughput.\nSee <a href=\"https://github.com/valkey-io/valkey/pull/477\">#477</a> for details.</li>\n<li><strong>Numerous Smaller Performance/Reliability Enhancements</strong>: Many under-the-hood\nimprovements ensure a smoother, more stable experience across the board.\nSee <a href=\"https://github.com/valkey-io/valkey/releases/tag/8.0.0-rc1\">release notes</a> for details.</li>\n</ul>\n<h2>Conclusion</h2>\n<p>Valkey 8.0 is a major update that offers improved performance, reliability, and\nobservability. Whether you are an experienced Valkey/Redis user or exploring\nit for the first time, this release provides significant advancements in in-memory\ndata storage. You can try out these enhancements today by downloading from \n<a href=\"https://github.com/valkey-io/valkey/releases/tag/8.0.0-rc1\">source</a> or using one \nof our <a href=\"https://hub.docker.com/r/valkey/valkey\">container images</a>. We would love \nto hear your thoughts on these new features and what you hope to see in the \nfuture from the Valkey project.</p>\n<p><strong>Important Note</strong>: The Valkey Over RDMA feature is currently experimental and\nmight change or be removed in future versions.</p>\n<p>We look forward to seeing what you achieve with Valkey 8.0!</p>\n",
    "slug": "valkey-8-0-0-rc1",
    "category": "news",
    "imageUrl": "/assets/media/blog/random-02.webp",
    "authorUsernames": [
      "pingxie",
      "madolson"
    ],
    "trending": false
  },
  {
    "title": "Using Bitnami's Valkey chart",
    "date": "2024-07-09T07:01:01.000Z",
    "excerpt": "An introductory example using Bitnami's Valkey chart",
    "content": "<p>Valkey is a high-performance key/value datastore that supports workloads such as caching, and message queues, supporting many data types including strings, numbers, hashes, bitmaps, and more. Valkey can run in standalone or cluster mode for replication and high availability.</p>\n<p><a href=\"https://bitnami.com/\">Bitnami</a> offers a number of secure, up-to-date, and easy to deploy charts for a number of popular open source applications.</p>\n<p>This blog will serve as a walk-through on how you can deploy and use the <a href=\"https://github.com/bitnami/charts/tree/main/bitnami/valkey\">Bitnami Helm chart for Valkey</a>.</p>\n<h1>Assumptions and prerequisites</h1>\n<p>Before starting the deployment, make sure that you have the following prerequisites:</p>\n<ul>\n<li>An operational Kubernetes cluster.</li>\n<li>An installed and configured kubectl CLI and Helm v3.x package manager. If you need help with these steps, check our article “<a href=\"https://docs.bitnami.com/kubernetes/get-started-kubernetes#step-3-install-kubectl-command-line\">Learn how to install kubectl and Helm v3.x.</a>”</li>\n<li>Optional: Access to <a href=\"https://app-catalog.vmware.com/catalog\">VMware Tanzu Application Catalog</a>.</li>\n</ul>\n<h1>Deploying the Bitnami package for the Valkey Helm chart</h1>\n<p>The sections below describe the steps to configure the deployment, get and deploy the Bitnami-package Valkey Helm chart, and obtain its external IP address to access the service.</p>\n<h2>Getting and deploying the Bitnami package for Valkey Helm chart</h2>\n<p>You can deploy the <a href=\"https://github.com/bitnami/charts/blob/main/LICENSE\">community</a> Bitnami-packaged Valkey Helm chart from the open source Bitnami Application Catalog. Alternatively, if you have access to an enterprise Tanzu Application Catalog instance, it can also be deployed from there.</p>\n<h3>Deploying the open source version of the chart through Bitnami Application Catalog</h3>\n<p>To deploy the chart in its namespace, run the following commands:</p>\n<pre><code class=\"language-bash\">$ kubectl create namespace valkey\n$ helm install myvalkey oci://registry-1.docker.io/bitnamicharts/valkey --set auth.enabled=true --set auth.password=test_pwd --namespace valkey\n</code></pre>\n<h3>Deploying the enterprise version of the chart through Tanzu Application Catalog</h3>\n<p>The following steps describe navigating the Tanzu Application Catalog and getting the instructions to deploy Valkey in your cluster. This example shows a Valkey chart built using Ubuntu 22 as the base OS image, but feel free to customize the chart depending on your needs.</p>\n<ol>\n<li>Navigate to <a href=\"https://app-catalog.vmware.com\">app-catalog.vmware.com</a> and sign in to your catalog with your VMware account.\n<img src=\"/content/blog/2024-06-27-using-bitnami-valkey-chart/images/using-bitnami-valkey-chart_1.png\" alt=\"Tanzu Library\" /></li>\n<li>In the My Applications section, search for Valkey and request it for your catalog. It is supported by Photon, Ubuntu, RHEL UBI, and Debian Linux distributions. On the next screen, you will find the instructions for deploying the chart on your cluster. Make sure that your cluster is up and running.</li>\n<li>Execute <strong>kubectl cluster-info</strong>, then run the commands you will find in the Consume your Helm chart section.</li>\n</ol>\n<p><img src=\"/content/blog/2024-06-27-using-bitnami-valkey-chart/images/using-bitnami-valkey-chart_2.png\" alt=\"Tanzu Application Catalog\" /></p>\n<p><img src=\"/content/blog/2024-06-27-using-bitnami-valkey-chart/images/using-bitnami-valkey-chart_3.png\" alt=\"Bitnami Package Content\" /></p>\n<p>After this, the steps for deploying the chart will be the same as the ones described in the following sections to deploy its community version.</p>\n<h2>Obtaining the external IP address and logging into Valkey</h2>\n<p>Wait for the deployment to complete and check that all <em>myvalkey</em> pods are Running.</p>\n<pre><code class=\"language-bash\">$ kubectl get pods,svc -n valkey\nNAME                      READY   STATUS    RESTARTS   AGE\npod/myvalkey-master-0     1/1     Running   0          4m41s\npod/myvalkey-replicas-0   1/1     Running   0          4m41s\npod/myvalkey-replicas-1   1/1     Running   0          3m59s\npod/myvalkey-replicas-2   1/1     Running   0          3m32s\n\nNAME                        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE\nservice/myvalkey-headless   ClusterIP   None            &lt;none&gt;        6379/TCP   4m41s\nservice/myvalkey-master     ClusterIP   10.110.225.33   &lt;none&gt;        6379/TCP   4m41s\nservice/myvalkey-replicas   ClusterIP   10.98.176.69    &lt;none&gt;        6379/TCP   4m41s\n</code></pre>\n<h2>Connecting using Valkey’s CLI client</h2>\n<p>To connect to the server, you can deploy a client pod.</p>\n<pre><code class=\"language-bash\">$ export VALKEY_PASSWORD=test_pwd\n$ kubectl run --namespace valkey valkey-client --restart=&#39;Never&#39;  --env VALKEY_PASSWORD=$VALKEY_PASSWORD  --image docker.io/bitnami/valkey:7.2.5-debian-12-r5 --command -- sleep infinity\n</code></pre>\n<p>You are now ready to attach to the pod and interact with the server using the CLI client.</p>\n<pre><code class=\"language-bash\">$ kubectl exec --tty -i valkey-client --namespace valkey -- bash\n$ REDISCLI_AUTH=&quot;$VALKEY_PASSWORD&quot; valkey-cli -h myvalkey-master\nmyvalkey-master:6379&gt; set test_key &quot;test_value&quot;\nOK\nmyvalkey-master:6379&gt; get test_key\n&quot;test_value&quot;\n</code></pre>\n<h2>Using a Python environment</h2>\n<p>The next example explains how to connect to Valkey using a Python script. To run the script, first, it is needed to provide a Python environment and install Python’s Redis package.</p>\n<pre><code class=\"language-bash\">$ kubectl run -it  python-redis --image=bitnami/python -- bash\nroot@python-redis:/app# pip install redis\nCollecting redis\nDownloading redis-5.0.6-py3-none-any.whl.metadata (9.3 kB)\nDownloading redis-5.0.6-py3-none-any.whl (252 kB)\n  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 252.0/252.0 kB 942.1 kB/s eta 0:00:00\nInstalling collected packages: redis\nSuccessfully installed redis-5.0.6\n</code></pre>\n<p>You are now ready to run the script.</p>\n<pre><code class=\"language-bash\">root@python-redis:/app# python\nPython 3.12.4 (main, Jun  7 2024, 04:30:17) [GCC 12.2.0] on linux\nType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.\n&gt;&gt;&gt; import redis\n&gt;&gt;&gt; creds_provider = redis.UsernamePasswordCredentialProvider(&quot;&quot;, &quot;test_pwd&quot;)\n&gt;&gt;&gt; user_connection = redis.Redis(host=&quot;myvalkey-master.valkey&quot;, port=6379, credential_provider=creds_provider)\n&gt;&gt;&gt;\n&gt;&gt;&gt; user_connection.ping()\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; user_connection.set(&#39;foo_key&#39;, &#39;bar&#39;)\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; user_connection.get(&#39;foo_key&#39;)\nb&#39;bar&#39;\n&gt;&gt;&gt;\n&gt;&gt;&gt; user_connection.hset(&#39;user-session:123&#39;, mapping={\n...     &#39;name&#39;: &#39;John&#39;,\n...     &quot;surname&quot;: &#39;Smith&#39;,\n...     &quot;company&quot;: &#39;Redis&#39;,\n...     &quot;age&quot;: 29\n... })\n4\n&gt;&gt;&gt;\n&gt;&gt;&gt; user_connection.hgetall(&#39;user-session:123&#39;)\n{b&#39;name&#39;: b&#39;John&#39;, b&#39;surname&#39;: b&#39;Smith&#39;, b&#39;company&#39;: b&#39;Redis&#39;, b&#39;age&#39;: b&#39;29&#39;}\n</code></pre>\n<h2>Using Valkey as a cache for WordPress</h2>\n<p>Valkey can also work as an object cache for a WordPress deployment. The following steps will show you how to set up this scenario.</p>\n<p>First, it is needed to create the configuration for the WordPress deployment, in this case, Valkey’s parameters and the user credential for the site administrator are set.</p>\n<pre><code class=\"language-bash\">$ cat &gt; settings.yaml &lt;&lt;EOF\nwordpressPassword: &quot;wp_pwd&quot;\nwordpressExtraConfigContent: |\n  define( &#39;WP_REDIS_HOST&#39;, &#39;myvalkey-master.valkey&#39; );\n  define( &#39;WP_REDIS_PORT&#39;, 6379 );\n  define( &#39;WP_REDIS_PASSWORD&#39;, &#39;test_pwd&#39; );\n  define( &#39;WP_REDIS_PREFIX&#39;, &#39;wp_redis&#39; );\nEOF\n</code></pre>\n<p>Once you have the settings parameters, it is time to deploy WordPress in its namespace</p>\n<pre><code class=\"language-bash\">$ kubectl create namespace wordpress\n$ helm install mywp bitnami/wordpress -f settings.yaml --namespace wordpress\n</code></pre>\n<p>Wait for the deployment to complete and check that the service has been assigned an external IP</p>\n<pre><code class=\"language-bash\">$ kubectl get svc --namespace wordpress\nNAME                   TYPE         CLUSTER-IP     EXTERNAL-IP  PORT(S)         AGE\nservice/mywp-mariadb   ClusterIP    10.106.11.191  &lt;none&gt;       3306/TCP                    74s\nservice/mywp-wordpress LoadBalancer 10.100.136.181 192.168.49.50   80:31116/TCP,443:31119/TCP   74s\n</code></pre>\n<p>Using the external IP, browse to the admin page and log in using the previously set credentials.</p>\n<p>You will be able to see the WordPress administrator UI where you can search for <code>Redis Object Cache</code> plugin, and then install and activate it. After enabling it, wait for some metrics to appear:</p>\n<p><img src=\"/content/blog/2024-06-27-using-bitnami-valkey-chart/images/using-bitnami-valkey-chart_4.png\" alt=\"Redis Object Cache Metrics\" /></p>\n<p>This is only a drop in the bucket of what can be done with Valkey. Thereafter you could easily integrate Valkey into your projects.</p>\n<h1>Support and resources</h1>\n<p>The Bitnami package for Valkey is available in both the community version, through the <a href=\"https://github.com/bitnami/charts/tree/main/bitnami/valkey/#installing-the-chart\">Bitnami GitHub repository</a>, as well as the enterprise version, <a href=\"https://app-catalog.vmware.com/catalog/f1242d16-218e-4773-8856-adcb2b2006e9/branch/1542f88a-71c4-42ed-b79c-89bd2063ac9a\">Tanzu Application Catalog</a>. Learn more about the differences between these two catalogs in this <a href=\"https://tanzu.vmware.com/content/blog/open-source-vs-enterprise-edition-of-vmware-bitnami-application-catalog\">blog post</a>.</p>\n<p>To solve the problems you may have with the Bitnami community packages—including deployment support, operational support, and bug fixes—please open an issue in the Bitnami <a href=\"https://github.com/bitnami/charts\">Helm charts</a> or <a href=\"https://github.com/bitnami/containers\">containers</a> GitHub repositories. Also, if you want to contribute to the catalog, feel free to send us a <a href=\"https://github.com/bitnami/containers/pulls\">pull request</a>, and the team will check it and guide you in the process for a successful merge.</p>\n<p>If you are interested in learning more about the Tanzu Application Catalog in general, check out the <a href=\"https://tanzu.vmware.com/application-catalog\">product webpage</a>.</p>\n",
    "slug": "using-bitnami-valkey-chart",
    "category": "news",
    "imageUrl": "/assets/media/blog/random-02.webp",
    "authorUsernames": [
      "rafariossaa"
    ],
    "trending": false
  },
  {
    "title": "What's new in Valkey for June 2024",
    "date": "2024-06-27T07:01:01.000Z",
    "excerpt": "What have people been saying about Valkey in June 2024? Read on to find out.",
    "content": "<p>What have people been saying since the <a href=\"/blog/may-roundup/\">last what&#39;s new post</a>? Read on to find out.</p>\n<h2>News and Press</h2>\n<p>Richard Speed from <a href=\"https://www.theregister.com/2024/06/19/valkey_picks_up_more_partners/\">The Register tells about how Valkey gained momentum with new backers</a>.\nMichael Larabel wrote a couple of stories on Phoronix: one on how <a href=\"https://www.phoronix.com/news/Fedora-Replacing-Redis-Valkey\">Valkey will be packaged in Fedora 41</a> and another on <a href=\"https://www.phoronix.com/news/Valkey-Redis-Fork-More-Backers\">Valkey&#39;s new backers</a>.</p>\n<h2>Valkey How-To</h2>\n<p>Abhishek Gupta gives a <a href=\"https://community.aws/content/2hx81ITCvDiWqrAz06SECOvepoa/getting-started-with-valkey-using-javascript\">tutorial on how to get started with Valkey on JavaScript (and LangChain)</a>.</p>\n<p>In less than 10 minutes Shantanu shows you how to install Valkey from source on Ubuntu.</p>\n<p>{{ youtube(id=&quot;T-tH1GC0omo&quot;) }}</p>\n<p>Percona published a couple of detailed how-to posts: Matthew Boehm detailed <a href=\"https://www.percona.com/blog/valkey-redis-setting-up-replication/\">setting up Valkey replication</a> and Anil Joshi <a href=\"https://www.percona.com/blog/valkey-redis-sharding-using-the-native-clustering-feature/\">covers Valkey sharding</a>.</p>\n<h2>Interviews and Podcasts</h2>\n<p>Swapnil from <a href=\"https://tfir.io/why-open-source-still-leads-the-way-despite-license-changes-ann-schlemmer/\">TFIR interviews Ann Schlemmer from Percona</a> about why open source still leads the way despite license changes.</p>\n<p>{{ youtube(id=&quot;D2G7kfAO37U&quot;) }}</p>\n<p>TSC member <a href=\"https://www.odbms.org/2024/06/on-the-open-source-valkey-project-qa-with-madelyn-olson/\">Madelyn Olson was interviewed by Roberto Zicari on ODBMS</a> and chatted with <a href=\"https://www.lastweekinaws.com/podcast/screaming-in-the-cloud/steering-through-open-source-waters-with-madelyn-olson/\">Corey Quinn about Valkey on the Screaming in the Cloud podcast</a>.</p>\n<p>{{ youtube(id=&quot;Pl-udfEPwtk&quot;) }}</p>\n<p>Robert and Courtney from the WooCommerce community chat about Valkey on the <a href=\"https://www.youtube.com/watch?v=E1hX1GZij_U\">Do the Woo Podcast</a>.</p>\n<p>{{ youtube(id=&quot;E1hX1GZij_U&quot;) }}</p>\n<h2>Want to feature your tutorial/article/meetup/video?</h2>\n<p>Add your own links to the <a href=\"https://github.com/valkey-io/valkey-io.github.io/pulls?q=is%3Apr+is%3Aopen+label%3Aroundup-post\">draft pull request open on the website GitHub repo</a>.\nYou can also submit your own content to be published directly on valkey.io by following the <a href=\"https://github.com/valkey-io/valkey-io.github.io/blob/main/CONTRIBUTING-BLOG-POST\">blog post contributing guide</a>.</p>\n",
    "slug": "whats-new-june-2024",
    "category": "news",
    "imageUrl": "/assets/media/blog/random-06.webp",
    "authorUsernames": [
      "kyledvs"
    ],
    "trending": false
  },
  {
    "title": "What's new in Valkey for May 2024",
    "date": "2024-05-24T07:01:01.000Z",
    "excerpt": "It's become clear that people want to talk about Valkey and have been publishing blog posts/articles fervently. Here you'll find a collection of all the post I'm aware of in the last few weeks.",
    "content": "<p>It&#39;s become clear that people want to talk about Valkey and have been publishing blog posts/articles fervently.\nHere you&#39;ll find a collection of all the post I&#39;m aware of in the last few weeks.</p>\n<h2>Percona</h2>\n<p>The kind folks over at Percona have been on an absolutely legendary streak of posting about Valkey.\nThey&#39;ve done a series on data types (<a href=\"https://www.percona.com/blog/valkey-redis-the-hash-datatype/\">Hashes</a>, <a href=\"https://www.percona.com/blog/valkey-redis-sets-and-sorted-sets/\">Sorted Sets</a>), <a href=\"https://www.percona.com/blog/valkey-redis-configuration-best-practices/\">best</a> and <a href=\"https://www.percona.com/blog/valkey-redis-not-so-good-practices/\">not-so-good practices</a>, <a href=\"https://www.percona.com/blog/hello-valkey-lets-get-started/\">getting started</a>, <a href=\"https://www.percona.com/blog/valkey-redis-replication-and-auto-failover-with-sentinel-service/\">replication/failover</a>, <a href=\"https://www.percona.com/blog/valkey-redis-configurations-and-persistent-setting-of-the-key-parameters/\">configurations/persistence</a>, and finally their own <a href=\"https://www.percona.com/blog/hello-valkey-lets-get-started/\">Valkey packages for DEB and RPM-based distros</a>.</p>\n<h2>Fedora Magazine</h2>\n<p>Yours truly wrote an article for <a href=\"https://fedoramagazine.org/how-to-move-from-redis-to-valkey/\">Fedora Magazine about using the <code>valkey-compat-redis</code> package to move to Valkey</a>.</p>\n<h2>Community.aws</h2>\n<p>Ricardo Ferreira put together a <a href=\"https://community.aws/content/2fdr6Vg8BiJS8jr8xsuQRRc0MD5/getting-started-with-valkey-using-docker-and-go\">walkthrough of using Valkey with Go on Docker</a>.</p>\n<h2>The New Stack</h2>\n<p>While Open Source Summit North America was last month, <a href=\"https://thenewstack.io/valkey-a-redis-fork-with-a-future/\">The New Stack published a blog post about Valkey</a> and accompany interview with project leaders, it&#39;s worth a watch and read.</p>\n<h2>Presentation: Digging into Valkey</h2>\n<p>On the subject of Open Source Summit, the talk I gave along side Madelyn Olson, <a href=\"https://youtu.be/3G6QgwIl-xs\">&quot;Digging into Valkey&quot; was posted as a video</a>.</p>\n<h2>Valkey Seattle IRL</h2>\n<p>The <a href=\"https://www.meetup.com/seattle-valkey/\">Seattle Valkey Meetup</a> is holding a <a href=\"https://www.meetup.com/seattle-valkey/events/301177195/\">Rust module workshop on June 5th</a>.\nA lot of folks will be in town for the Contributor Summit, so this meet up is bound to  be flush with Valkey experts.\nDon&#39;t miss it.</p>\n<h2>Want to add your tutorial/article/meetup/video to a future roundup?</h2>\n<p>This is the first in a series of roundups on Valkey content.\nThe plan is to keep an <a href=\"https://github.com/valkey-io/valkey-io.github.io/issues?q=is%3Adraft+label%3Aroundup-post+\">draft pull request open on the website GitHub repo</a> where you can contribute your own content.</p>\n",
    "slug": "may-roundup",
    "category": "news",
    "imageUrl": "/assets/media/blog/random-06.webp",
    "authorUsernames": [
      "kyledvs"
    ],
    "trending": false
  },
  {
    "title": "Valkey Modules 101",
    "date": "2024-05-01T07:01:01.000Z",
    "excerpt": "The idea of modules is to allow adding extra features (such as new commands and data types) to Valkey without making changes to the core code.",
    "content": "<h2>What are Valkey modules?</h2>\n<p>The idea of modules is to allow adding extra features (such as new commands and data types) to Valkey without making changes to the core code.\nModules are a special type of code distribution called a shared library, which can be loaded by other programs at runtime and executed.\nModules can be written in C or other languages that have C bindings.\nIn this article we will go through the process of building simple modules in C and Rust (using the Valkey Module Rust SDK).\nThis article expects the audience to be at least somewhat familiar with git, C, Rust and Valkey.</p>\n<h2>Hello World module in C</h2>\n<p>If we clone the Valkey repo by running <code>git clone git@github.com:valkey-io/valkey.git</code> we will find numerous examples in <code>src/modules</code>.\nLet&#39;s create a new file <code>module1.c</code> in the same folder.</p>\n<pre><code class=\"language-c\">#include &quot;../valkeymodule.h&quot;\n\nint hello(ValkeyModuleCtx *ctx, ValkeyModuleString **argv, int argc) {\n    VALKEYMODULE_NOT_USED(argv);\n    VALKEYMODULE_NOT_USED(argc);\n    return ValkeyModule_ReplyWithSimpleString(ctx, &quot;world1&quot;);\n}\n\nint ValkeyModule_OnLoad(ValkeyModuleCtx *ctx, ValkeyModuleString **argv, int argc) {\n    VALKEYMODULE_NOT_USED(argv);\n    VALKEYMODULE_NOT_USED(argc);\n    if (ValkeyModule_Init(ctx,&quot;module1&quot;,1,VALKEYMODULE_APIVER_1) \n        == VALKEYMODULE_ERR) return VALKEYMODULE_ERR;\n    if (ValkeyModule_CreateCommand(ctx,&quot;module1.hello&quot;, hello,&quot;&quot;,0,0,0) \n        == VALKEYMODULE_ERR) return VALKEYMODULE_ERR;\n    return VALKEYMODULE_OK;\n}\n</code></pre>\n<p>Here we are calling <code>ValkeyModule_OnLoad</code> C function (required by Valkey) to initialize <code>module1</code> using <code>ValkeyModule_Init</code>.\nThen we use <code>ValkeyModule_CreateCommand</code> to create a Valkey command <code>hello</code> which uses C function <code>hello</code> and returns <code>world1</code> string.\nIn future blog posts we will expore these areas at greater depth.</p>\n<p>Now we need to update <code>src/modules/Makefile</code></p>\n<pre><code class=\"language-makefile\">all: ... module1.so\n\nmodule1.xo: ../valkeymodule.h\n\nmodule1.so: module1.xo\n    $(LD) -o $@ $^ $(SHOBJ_LDFLAGS) $(LIBS) -lc\n</code></pre>\n<p>Run <code>make module1.so</code> inside <code>src/modules</code> folder.\nThis will compile our module in the <code>src/modules</code> folder.</p>\n<h2>Hello World module in Rust</h2>\n<p>We will create a new Rust package by running <code>cargo new --lib module2</code> in bash.\nInside the <code>module2</code> folder we will have <code>Cargo.toml</code> and <code>src/lib.rs</code> files.\nTo install the valkey-module SDK run <code>cargo add valkey-module</code> inside <code>module2</code> folder.\nAlternativley we can add <code>valkey-module = &quot;0.1.0</code> in <code>Cargo.toml</code> under <code>[dependencies]</code>.\nRun <code>cargo build</code> and it will create or update the <code>Cargo.lock</code> file.</p>\n<p>Modify <code>Cargo.toml</code> to specify the crate-type to be &quot;cdylib&quot;, which will tell cargo to build the target as a shared library.\nRead <a href=\"https://doc.rust-lang.org/reference/linkage.html\">Rust docs</a> to understand more about <code>crate-type</code>.</p>\n<pre><code>[lib]\ncrate-type = [&quot;cdylib&quot;]\n</code></pre>\n<p>Now in <code>src/lib.rs</code> replace the existing code with the following:</p>\n<pre><code class=\"language-rust\">#[macro_use]\nextern crate valkey_module;\n\nuse valkey_module::{Context, ValkeyResult, ValkeyString, ValkeyValue};\n\nfn hello(_ctx: &amp;Context, _args: Vec&lt;ValkeyString&gt;) -&gt; ValkeyResult {\n    Ok(ValkeyValue::SimpleStringStatic(&quot;world2&quot;))\n}\n\nvalkey_module! {\n    name: &quot;module2&quot;,\n    version: 1,\n    allocator: (valkey_module::alloc::ValkeyAlloc, valkey_module::alloc::ValkeyAlloc),\n    data_types: [],\n    commands: [\n        [&quot;module2.hello&quot;, hello, &quot;&quot;, 0, 0, 0],\n    ]\n}\n</code></pre>\n<p>Rust syntax is a bit different than C but we are creating <code>module2</code> with command <code>hello</code> that returns <code>world2</code> string.\nWe are using the external crate <code>valkey_module</code> with <a href=\"https://doc.rust-lang.org/book/ch19-06-macros.html\">Rust macros</a> and passing it variables like <code>name</code> and <code>version</code>.\nSome variables like <code>data_types</code> and <code>commands</code> are arrays and we can pass zero, one or many values.\nSince we are not using ctx or args we prefix them with <code>_</code> (Rust convention) instead of <code>VALKEYMODULE_NOT_USED</code> as we did in C.</p>\n<p>Run <code>cargo build</code> in the root folder.\nWe will now see <code>target/debug/libmodule2.dylib</code> (on macOS).\nThe build will produce *.so files on Linux and *.dll files on Windows.</p>\n<h2>Run Valkey server with both modules</h2>\n<p>Go back into the Valkey repo folder and run <code>make</code> to compile the Valkey code.\nThen add these lines to the bottom of the <code>valkey.conf</code> file.</p>\n<pre><code>loadmodule UPDATE_PATH_TO_VALKEY/src/modules/module1.so\nloadmodule UPDATE_PATH_TO_MODULE2/target/debug/libmodule2.dylib\n</code></pre>\n<p>and run <code>src/valkey-server valkey.conf</code>.\nYou will see these messages in the log output.</p>\n<pre><code>Module &#39;module1&#39; loaded from UPDATE_PATH_TO_VALKEY/src/modules/module1.so\n...\nModule &#39;module2&#39; loaded from UPDATE_PATH_TO_MODULE2/target/debug/libmodule2.dylib\n</code></pre>\n<p>Then use <code>src/valkey-cli</code> to connect.</p>\n<pre><code class=\"language-bash\">src/valkey-cli -3\n127.0.0.1:6379&gt; module list\n1) 1# &quot;name&quot; =&gt; &quot;module2&quot;\n   2# &quot;ver&quot; =&gt; (integer) 1\n   3# &quot;path&quot; =&gt; &quot;UPDATE_PATH_TO_MODULE2/target/debug/libmodule2.dylib&quot;\n   4# &quot;args&quot; =&gt; (empty array)\n2) 1# &quot;name&quot; =&gt; &quot;module1&quot;\n   2# &quot;ver&quot; =&gt; (integer) 1\n   3# &quot;path&quot; =&gt; &quot;UPDATE_PATH_TO_VALKEY/src/modules/module1.so&quot;\n   4# &quot;args&quot; =&gt; (empty array)\n127.0.0.1:6379&gt; module1.hello\nworld1\n127.0.0.1:6379&gt; module2.hello\nworld2\n</code></pre>\n<p>We can now run both modules side by side and if we modify either C or RS file, recompile the code and restart <code>valkey-server</code> we will get the new functionality.</p>\n<p>As an alternative to specifying modules in <code>valkey.conf</code> file, we can use <code>MODULE LOAD</code> and <code>UNLOAD</code> from <code>valkey-cli</code> to update the server.\nFirst specify <code>enable-module-command yes</code> in <code>valkey.conf</code> and restart <code>valkey-server</code>.\nThis enables us to update our module code, recompile it and reload it at runtime.</p>\n<pre><code>127.0.0.1:6379&gt; module load UPDATE_PATH_TO_VALKEY/src/modules/module1.so\nOK\n127.0.0.1:6379&gt; module list\n1) 1# &quot;name&quot; =&gt; &quot;module1&quot;\n   2# &quot;ver&quot; =&gt; (integer) 1\n   3# &quot;path&quot; =&gt; &quot;UPDATE_PATH_TO_VALKEY/src/modules/module1.so&quot;\n   4# &quot;args&quot; =&gt; (empty array)\n127.0.0.1:6379&gt; module unload module1\nOK\n127.0.0.1:6379&gt; module list\n(empty array)\n127.0.0.1:6379&gt; \n</code></pre>\n<p>Please stay tuned for more articles in the future as we explore the possibilities of Valkey modules and where using C or Rust makes sense.</p>\n<h2>Usefull links</h2>\n<ul>\n<li><a href=\"https://github.com/valkey-io/valkey\">Valkey repo</a></li>\n<li><a href=\"https://github.com/valkey-io/valkeymodule-rs\">Valkey Rust SDK</a></li>\n<li><a href=\"https://code.visualstudio.com/docs/languages/rust\">Rust in VS Code</a></li>\n</ul>\n",
    "slug": "modules-101",
    "category": "news",
    "imageUrl": "/assets/media/blog/default.webp",
    "authorUsernames": [
      "dmitrypol"
    ],
    "trending": false
  },
  {
    "title": "Valkey 7.2.5 GA is out!",
    "date": "2024-04-16T07:01:01.000Z",
    "excerpt": "Exciting times!I'm pleased to announce that you can start using the first generally available, stable Valkey release today.",
    "content": "<p>Exciting times!</p>\n<p>I&#39;m pleased to announce that you can start using the first generally available, stable Valkey release today.\nCheck out the <a href=\"/download/releases/v7-2-5\">release page for 7.2.5</a>.</p>\n<p>This release maintains the same protocol, API, return values, and data file formats with the last open source release of Redis (7.2.4).</p>\n<p>You can <a href=\"https://github.com/valkey-io/valkey/releases/tag/7.2.5\">build it from source</a> or <a href=\"https://hub.docker.com/r/valkey/valkey/\">pull it from Valkey’s official Docker Hub</a>.\nValkey’s release candidates are available in Fedora and EPEL and the new release will be available once the community updates the packages.</p>\n",
    "slug": "valkey-7-2-5-out",
    "category": "news",
    "imageUrl": "/assets/media/blog/default.webp",
    "authorUsernames": [
      "kyledvs"
    ],
    "trending": false
  },
  {
    "title": "SET first-blog-post \\\"Hello, world\\\"",
    "date": "2024-04-11T07:01:01.000Z",
    "excerpt": "Welcome! For the inaugural blog post on valkey.io, I’d like to recap the story so far, what to look forward to, and then describe how this blog works.",
    "content": "<p>Welcome!\nFor the inaugural blog post on valkey.io, I’d like to recap the story so far, what to look forward to, and then describe how this blog works.</p>\n<h2>How do you describe an open source whirlwind?</h2>\n<p>I would describe it like this: first <a href=\"https://github.com/redis/redis/pull/13157\">a license change</a>, the <a href=\"https://github.com/valkey-io/valkey/commit/38632278fd06fe186f7707e4fa099f666d805547#diff-b335630551682c19a781afebcf4d07bf978fb1f8ac04c6bf87428ed5106870f5\">establishment of PlaceholderKV</a>, a new name and <a href=\"https://www.linuxfoundation.org/press/linux-foundation-launches-open-source-valkey-community\">formation of Valkey within the Linux Foundation</a>, <a href=\"https://github.com/valkey-io/valkey/compare/redis-7.2.4...7.2.4-rc1\">hundreds of code updates</a> by community members from around the world, and <a href=\"https://github.com/valkey-io/valkey/releases/tag/7.2.4-rc1\">a release candidate</a>.\nAll within the span of about three weeks.</p>\n<h2>Only the start</h2>\n<p>Out of this initial flurry of activity emerges a project that is sure to have a long history.\nThis blog will cover the project over time by describing what’s new, what to look forward to, and how you can explore the full extent of Valkey.</p>\n<h2>For the community, with the community</h2>\n<p>Like the Valkey project itself, this blog is not a singular effort of one company but rather a community effort, <a href=\"https://github.com/valkey-io/valkey-io.github.io/\">built in the open with full transparency</a>.\nYou want to write about a topic on the blog?\n<a href=\"https://github.com/valkey-io/valkey-io.github.io/fork\">Fork it and make a pull request.</a>\nYou want to help edit or review a post?\n<a href=\"https://github.com/valkey-io/valkey-io.github.io/issues\">Do a code review.</a>\nProblem with a post?\n<a href=\"https://github.com/valkey-io/valkey-io.github.io/issues/new?assignees=&labels=bug%2C+untriaged&projects=&template=bug_template.md&title=%5BBUG%5D\">Create an issue.</a>\nFeel like something is missing?\n<a href=\"https://github.com/valkey-io/valkey-io.github.io/issues/new?assignees=&labels=enhancement&projects=&template=feature_template.md&title=\">Make a feature request.</a></p>\n<h2>What’s next?</h2>\n<p>Stay tuned for trip reports from Valkey’s first conferences then information about the first GA release.\nIt’s only going to get more exciting from here.</p>\n",
    "slug": "hello-world",
    "category": "news",
    "imageUrl": "/assets/media/blog/default.webp",
    "authorUsernames": [
      "kyledvs"
    ],
    "trending": false
  }
] as const;

export const blogPosts: BlogPost[] = blogPostsRaw.map(post => ({
  ...post,
  imageUrl: getImagePath(post.imageUrl),
  content: import.meta.env.PROD ? post.content.replace(/src="\/\/src\//g, 'src="/') : post.content,
  category: post.category as 'tutorials' | 'news' | 'case-studies',
  authors: post.authorUsernames.map(username => 
    authors.find(a => a.username === username) || {
      name: 'Unknown Author',
      username,
      bio: 'No bio available',
      imageUrl: '/images/authors/default.jpg',
      role: 'Contributor'
    }
  )
}));

export const blogDigest = blogPosts.map(({ title, date, excerpt, slug, category, imageUrl, authors, trending }) => ({
  title,
  date,
  excerpt,
  slug,
  category,
  imageUrl,
  authors,
  trending
}));
